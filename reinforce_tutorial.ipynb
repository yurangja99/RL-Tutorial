{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE Tutorial\n",
    "\n",
    "Tutorial network to construct pipeline to train and test REINFORCE agent. The agent has been trained and tested on gymnasium environments. \n",
    "\n",
    "## REINFORCE\n",
    "- Model-free\n",
    "- Policy-based\n",
    "- On-policy\n",
    "\n",
    "## Tasks\n",
    "- CartPole-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "In this notebook, we use CarPole-v1 environment from gymnasium. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical, Normal\n",
    "\n",
    "# define environment (editable)\n",
    "envname = \"CartPole-v1\"\n",
    "env = gym.make(envname)\n",
    "\n",
    "# plot settings\n",
    "is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "  from IPython import display\n",
    "\n",
    "# device setting (cpu or cuda!)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "- `NUM_EPISODES`: number of training episodes. \n",
    "- `TEST_EPISODES`: number of test episodes during training. \n",
    "- `TEST_FREQ`: testing frequency. \n",
    "- `GAMMA`: discount factor when calculating estimated goal. \n",
    "- `LR`: learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperParameter = namedtuple(\"HyperParameter\", (\n",
    "  \"NUM_EPISODES\",\n",
    "  \"TEST_EPISODES\",\n",
    "  \"TEST_FREQ\",\n",
    "  \"GAMMA\",\n",
    "  \"LR\"\n",
    "))\n",
    "\n",
    "# editable\n",
    "hp = HyperParameter(\n",
    "  NUM_EPISODES=600,\n",
    "  TEST_EPISODES=5,\n",
    "  TEST_FREQ=100,\n",
    "  GAMMA=0.99,\n",
    "  LR=0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network\n",
    "\n",
    "In this notebook, we use simple environment that have some real numbers as observations and (1) few discrete actions or (2) one continuous action. So, we use simple fully-connected network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePolicyNetwork(nn.Module):\n",
    "  \"\"\"\n",
    "  Fully-connected policy network. \n",
    "  \"\"\"\n",
    "  def __init__(self, dim_observation: tuple, action_space: Discrete | Box):\n",
    "    \"\"\"\n",
    "    n_observations input channels and n_actions output channels. \n",
    "    \"\"\"\n",
    "    super(SimplePolicyNetwork, self).__init__()\n",
    "    assert len(dim_observation) == 1\n",
    "    \n",
    "    # feature extraction\n",
    "    self.seqmodel = nn.Sequential(\n",
    "      nn.Linear(dim_observation[0], 128), \n",
    "      nn.ReLU(),\n",
    "      nn.Linear(128, 128), \n",
    "      nn.ReLU(),\n",
    "    )\n",
    "    \n",
    "    # action can be discrete or continuous\n",
    "    self.discrete_action = isinstance(action_space, Discrete)\n",
    "    if self.discrete_action:\n",
    "      # discrete action: softmax output\n",
    "      self.action_prob = nn.Sequential(\n",
    "        nn.Linear(128, action_space.n), \n",
    "        nn.Softmax(dim=-1)\n",
    "      )\n",
    "    else:\n",
    "      # continuous action: vector of float\n",
    "      assert len(action_space.shape) == 1\n",
    "      self.mean = nn.Linear(128, action_space.shape[0])\n",
    "      self.log_std = nn.Linear(128, action_space.shape[0])\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Forward with relu activation. \n",
    "    \"\"\"\n",
    "    x = self.seqmodel(x)\n",
    "    if self.discrete_action:\n",
    "      # discrete action\n",
    "      return self.action_prob(x)\n",
    "    else:\n",
    "      # continuous action\n",
    "      mean = self.mean(x)\n",
    "      log_std = self.log_std(x)\n",
    "      log_std = torch.clamp(log_std, min=-20, max=2)\n",
    "      std = torch.exp(log_std)\n",
    "      return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Utility Functions\n",
    "\n",
    "- `select_action`: select agent's action using policy network. \n",
    "- `save_plot`: plot objective func, average frames, and score. \n",
    "- `save_model`: store model, hyperparameters, and training info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy network\n",
    "dim_observation = env.observation_space.shape\n",
    "action_space = env.action_space\n",
    "discrete_action = isinstance(env.action_space, Discrete)\n",
    "\n",
    "policy_net = SimplePolicyNetwork(dim_observation, action_space).to(device)\n",
    "\n",
    "# adamw optimizer\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=hp.LR, amsgrad=True)\n",
    "\n",
    "# save directory\n",
    "start_datetime = datetime.now()\n",
    "dirname = start_datetime.strftime(\"%Y%m%d-%H%M%S\")\n",
    "path = os.path.join(os.getcwd(), \"reinforce_tutorial\", dirname)\n",
    "\n",
    "# training variables\n",
    "train_objectives = []\n",
    "train_frames = []\n",
    "train_scores = []\n",
    "test_frames = []\n",
    "test_scores = []\n",
    "steps = 0\n",
    "\n",
    "def select_action(state: np.ndarray):\n",
    "  \"\"\"\n",
    "  Select agent's action using policy network. \n",
    "  Don't use torch.no_grad() because REINFORCE is on-policy. \n",
    "  \"\"\"\n",
    "  assert state.shape[0] == 1\n",
    "  if discrete_action:\n",
    "    # get probabilities for each action\n",
    "    probs = policy_net(state)\n",
    "    # sample one action\n",
    "    dist = Categorical(probs)\n",
    "    action = dist.sample()\n",
    "    # return with log probabilities\n",
    "    return action.item(), dist.log_prob(action)\n",
    "  else:\n",
    "    # get mean and std\n",
    "    mean, std = policy_net(state)\n",
    "    # sample action from normal distribution (squeeze to [-1, 1])\n",
    "    dist = Normal(mean, std)\n",
    "    action = dist.sample()\n",
    "    action = torch.tanh(action)\n",
    "    # return with log probabilities\n",
    "    return action[0].cpu().numpy(), torch.sum(dist.log_prob(action), dim=-1, keepdim=False)\n",
    "\n",
    "def save_plot():\n",
    "  \"\"\"\n",
    "  Plot objective func, average frames, and score and save the figures. \n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(16, 12))\n",
    "  plt.clf()\n",
    "  plt.ion()\n",
    "  \n",
    "  plt.subplot(2, 2, 1)\n",
    "  plt.title(\"\")\n",
    "  plt.xlabel(\"\")\n",
    "  plt.ylabel(\"\")\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  \n",
    "  plt.subplot(2, 2, 2)\n",
    "  plt.title(\"J(θ)\")\n",
    "  plt.xlabel(\"Episode\")\n",
    "  plt.ylabel(\"J(θ)\")\n",
    "  plt.plot(*zip(*train_objectives), label=\"train\")\n",
    "  if len(train_objectives) >= 100:\n",
    "    x, y = zip(*train_objectives)\n",
    "    y = torch.mean(torch.tensor(y).unfold(0, 100, 1), dim=1)\n",
    "    plt.plot(x[99:], y, label=\"train-avg100\")\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  \n",
    "  plt.subplot(2, 2, 3)\n",
    "  plt.title(\"# of Frames\")\n",
    "  plt.xlabel(\"Episode\")\n",
    "  plt.ylabel(\"# of Frames\")\n",
    "  plt.plot(*zip(*train_frames), label=\"train\")\n",
    "  plt.plot(*zip(*test_frames), label=\"test\")\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  \n",
    "  plt.subplot(2, 2, 4)\n",
    "  plt.title(\"Score\")\n",
    "  plt.xlabel(\"Episode\")\n",
    "  plt.ylabel(\"Score\")\n",
    "  plt.plot(*zip(*train_scores), label=\"train\")\n",
    "  plt.plot(*zip(*test_scores), label=\"test\")\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  \n",
    "  plt.ioff()\n",
    "  plt.savefig(os.path.join(path, \"plot.png\"))\n",
    "  \n",
    "  if is_ipython:\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "def save_model():\n",
    "  \"\"\"\n",
    "  Save model, hyperparameters, and training info.\n",
    "  \"\"\"\n",
    "  # save model\n",
    "  torch.save({\n",
    "    \"policy_net\": policy_net.state_dict()\n",
    "  }, os.path.join(path, \"model.pt\"))\n",
    "  \n",
    "  # save hyperparameters\n",
    "  with open(os.path.join(path, \"hparam.json\"), \"w\") as w:\n",
    "    json.dump(hp._asdict(), w, indent=2)\n",
    "  \n",
    "  # save training info\n",
    "  with open(os.path.join(path, \"info.json\"), \"w\") as w:\n",
    "    json.dump(dict([\n",
    "      (\"env\", envname), \n",
    "      (\"test_frames\", test_frames), \n",
    "      (\"test_scores\", test_scores), \n",
    "      (\"steps\", steps), \n",
    "      (\"training_time\", (datetime.now() - start_datetime).seconds)\n",
    "    ]), w, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Testing Function\n",
    "\n",
    "- `test_model`: run policy model in the environment with trained policy network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "  \"\"\"\n",
    "  Test policy model and return the result in training variables. \n",
    "  \"\"\"\n",
    "  # testing variables\n",
    "  frames = []\n",
    "  scores = []\n",
    "  \n",
    "  # repeat for TEST_EPISODES episodes\n",
    "  for _ in range(1, hp.TEST_EPISODES + 1):\n",
    "    # initialize environment and state\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(np.array(state), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "    score = 0\n",
    "    \n",
    "    # start an episode\n",
    "    for frame in count():\n",
    "      # select action\n",
    "      with torch.no_grad():\n",
    "        action, _ = select_action(state)\n",
    "      \n",
    "      # act to next state\n",
    "      observation, reward, terminated, truncated, _ = env.step(action)\n",
    "      score += reward\n",
    "      done = terminated or truncated\n",
    "      \n",
    "      # update state\n",
    "      state = torch.tensor(np.array(observation), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "      # check end condition\n",
    "      if done:\n",
    "        frames.append(frame)\n",
    "        scores.append(score)\n",
    "        break\n",
    "      \n",
    "  # add to training variables\n",
    "  return np.mean(np.array(frames)), np.mean(np.array(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In training, simulate agent in the environment to create trajectory, and trains policy network. \n",
    "\n",
    "Object of REINFORCE algorithm is maximizing objective function, which is defined as below. \n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "J_i(\\theta_i) = E_{s0 \\sim p0} \\left[ v_{\\pi_{\\theta_i}}(s_0) \\right] = E_{\\tau \\sim \\pi_\\theta} \\left[ G(\\tau) \\right] \\notag \\\\\n",
    "\\text{where } \\tau = S_0, A_0, R_0, S_1, ..., S_{T-1}, A_{T-1}, R_{T-1}, S_T \\notag \\\\\n",
    "\\text{where } G(\\tau) = R_1 + \\gamma R_2 + ... + \\gamma^{T-1} R_T \\notag\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "We can get objective function using some mathematical tricks. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\theta J(\\theta) &= \\nabla_\\theta E_{\\tau \\sim \\pi_\\theta} \\left[ G(\\tau) \\right] \\\\ \n",
    "&= E_{\\tau \\sim \\pi_\\theta} \\left[ \\nabla_\\theta \\ln p(\\tau|\\pi_\\theta) G(\\tau) \\right] \\\\ \n",
    "&= E_{\\tau \\sim \\pi_\\theta} \\left[ G(\\tau) \\sum_{t=0}^T \\nabla_\\theta \\ln \\pi(A_t|S_t;\\pi_\\theta) \\right] \\\\ \n",
    "&= E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\gamma^t R_t \\sum_{t=0}^T \\nabla_\\theta \\ln \\pi(A_t|S_t;\\pi_\\theta) \\right] \\\\\n",
    "&= E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\left( \\left( \\sum_{k=t}^{T-1} \\gamma^k R_k \\right) \\nabla_\\theta \\ln \\pi(A_t|S_t;\\pi_\\theta) \\right) \\right] \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In this point, we approximate $ \\gamma $-included term to prevent reward vanishing. \n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\sum_{k=t}^{T-1} \\gamma^k R_k \n",
    "= \\gamma^t \\sum_{k=t}^{T-1}  \\gamma^{k-t} R_k \n",
    "= \\gamma^t G_t\n",
    "\\cong G_t \\notag \\\\\n",
    "\\nabla_\\theta J(\\theta) \n",
    "\\cong E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T G_t \\nabla_\\theta \\ln \\pi(A_t|S_t;\\pi_\\theta) \\right] \\notag\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "So, we update parameters as below. \n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^T G_t \\nabla_\\theta \\ln \\pi(A_t|S_t;\\pi_\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training directory\n",
    "os.makedirs(path)\n",
    "\n",
    "for episode in tqdm(range(1, hp.NUM_EPISODES + 1)):\n",
    "  # initialize environment and state\n",
    "  state, _ = env.reset()\n",
    "  state = torch.tensor(np.array(state), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "  score = 0\n",
    "  reward_history = [] # list of scalars\n",
    "  log_prob_history = [] # list of (1,) tensors\n",
    "  \n",
    "  # start an episode\n",
    "  for frame in count():\n",
    "    # select action\n",
    "    action, log_prob = select_action(state)\n",
    "    steps += 1\n",
    "    \n",
    "    # act to next state\n",
    "    observation, reward, terminated, truncated, _ = env.step(action)\n",
    "    score += reward\n",
    "    reward_history.append(reward)\n",
    "    log_prob_history.append(log_prob)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    # update state to next state\n",
    "    state = torch.tensor(np.array(observation), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "    \n",
    "    # check end condition\n",
    "    if done:\n",
    "      train_scores.append((episode, score))\n",
    "      train_frames.append((episode, frame))\n",
    "      break\n",
    "  \n",
    "  # get list of G_t\n",
    "  g_history = []\n",
    "  r = 0\n",
    "  for i in range(len(reward_history) - 1, -1, -1):\n",
    "    r = reward_history[i] + r * hp.GAMMA\n",
    "    g_history.insert(0, r)\n",
    "  g_history = torch.tensor(g_history, device=device)\n",
    "  \n",
    "  # normalize g_history\n",
    "  train_objectives.append((episode, g_history[0].item()))\n",
    "  g_history = (g_history - torch.mean(g_history)) / torch.std(g_history)\n",
    "  \n",
    "  # get list of log probs of actions\n",
    "  log_prob_history = torch.cat(log_prob_history).to(device=device)\n",
    "  \n",
    "  # gradient ascent\n",
    "  loss = -torch.sum(torch.mul(g_history, log_prob_history))\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  \n",
    "  if episode % hp.TEST_FREQ == 0:\n",
    "    mean_frame, mean_score = test_model()\n",
    "    test_frames.append((episode, mean_frame))\n",
    "    test_scores.append((episode, mean_score))\n",
    "    save_plot()\n",
    "    save_model()\n",
    "\n",
    "save_plot()\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "In this block, trained agent plays in the environment. We can see rendered environment played by the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(envname, render_mode=\"human\")\n",
    "\n",
    "scores = []\n",
    "\n",
    "# if you want to load from trained model, edit this (editable)\n",
    "load_dirname = None\n",
    "\n",
    "if load_dirname is not None:\n",
    "  # load models\n",
    "  path = os.path.join(os.getcwd(), \"reinforce_tutorial\", load_dirname)\n",
    "  checkpoint = torch.load(os.path.join(path, \"model.pt\"), map_location=device)\n",
    "  \n",
    "  policy_net.load_state_dict(checkpoint[\"policy_net\"])\n",
    "\n",
    "# repeat for TEST_EPISODES episodes\n",
    "for episode in range(1, hp.TEST_EPISODES + 1):\n",
    "  # initialize environment and state\n",
    "  state, _ = env.reset()\n",
    "  state = torch.tensor(np.array(state), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "  score = 0\n",
    "  \n",
    "  # start an episode\n",
    "  for _ in count():\n",
    "    # select greedy action\n",
    "    with torch.no_grad():\n",
    "      action, _ = select_action(state)\n",
    "    \n",
    "    # act to next state\n",
    "    observation, reward, terminated, truncated, _ = env.step(action)\n",
    "    score += reward\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    # update state\n",
    "    state = torch.tensor(np.array(observation), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # check end condition\n",
    "    if done:\n",
    "      print(f\"Episode {episode}: {score}\")\n",
    "      scores.append(score)\n",
    "      break\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(f\"Average: {sum(scores) / hp.TEST_EPISODES}\")\n",
    "print(f\"Max: {max(scores)}\")\n",
    "print(f\"Min: {min(scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
