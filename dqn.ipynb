{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN-based Algorithm Tutorial\n",
    "\n",
    "Notebook to construct pipeline to train and test DQN-based algorithms. There are advanced algorithms from DQN, and you can choose one of the options below. \n",
    "\n",
    "- PER (Prioritized Experience Replay)\n",
    "- Double DQN\n",
    "- Dueling DQN\n",
    "- Multi-step learning\n",
    "- Distributional RL\n",
    "- Noisy Networks\n",
    "- Rainbow (Apply all of the above)\n",
    "\n",
    "The agent have been trained and tested on gymnasium graphical environment - input will be pixels. \n",
    "\n",
    "## Tasks\n",
    "- ALE/Pong-v5\n",
    "- ALE/Breakout-v5\n",
    "- ALE/Enduro-v5\n",
    "- ALE/DemonAttack-v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "In this notebook, we use Gymnasium Atari environments that provides graphical observation. \n",
    "\n",
    "The agent gets F x H x W state (when grayscale applied) as an input and returns $ Q(s, a) $ as an output. (F: # of frames, H: height, W: width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from gymnasium.wrappers.atari_preprocessing import AtariPreprocessing\n",
    "from gymnasium.wrappers.frame_stack import FrameStack\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define environment (editable)\n",
    "envname = \"ALE/Pong-v5\"\n",
    "#envname = \"ALE/Breakout-v5\"\n",
    "#envname = \"ALE/Enduro-v5\"\n",
    "#envname = \"ALE/DemonAttack-v5\"\n",
    "\n",
    "def preprocess_env(env: gym.Env):\n",
    "  \"\"\"\n",
    "  Preprocess atari environment. \n",
    "  \"\"\"\n",
    "  env = AtariPreprocessing(\n",
    "    env,\n",
    "    frame_skip=1,\n",
    "    screen_size=84,\n",
    "    terminal_on_life_loss=False,\n",
    "    grayscale_obs=True,\n",
    "    grayscale_newaxis=False,\n",
    "    scale_obs=True\n",
    "  )\n",
    "  env = FrameStack(env, num_stack=4)\n",
    "  return env\n",
    "\n",
    "env = gym.make(envname)\n",
    "env = preprocess_env(env)\n",
    "\n",
    "# plot settings\n",
    "is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "  from IPython import display\n",
    "\n",
    "# device setting (cpu or cuda!)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Training Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Option = namedtuple(\"Option\", (\n",
    "  \"PER\", \n",
    "  \"DDQN\", \n",
    "  \"DUELING\", \n",
    "  \"MULTISTEP\", \n",
    "  \"DISTRIBUTIONAL\", \n",
    "  \"NOISY\"\n",
    "))\n",
    "\n",
    "# editable\n",
    "op = Option(\n",
    "  PER=True, \n",
    "  DDQN=True, \n",
    "  DUELING=True, \n",
    "  MULTISTEP=True, \n",
    "  DISTRIBUTIONAL=True, \n",
    "  NOISY=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "- `NUM_EPISODES`: number of training episodes. \n",
    "- `TEST_EPISODES`: number of test episodes during training. \n",
    "- `TEST_FREQ`: testing frequency. \n",
    "- `MEMORY_CAPACITY`: maximum capacity of replay memory. \n",
    "- `BATCH_SIZE`: size of sampled transitions for training. \n",
    "- `GAMMA`: discount factor when calculating estimated goal. \n",
    "- `EPS_START`: starting epsilon value in e-greedy policy. \n",
    "- `EPS_END`: final epsilon value in e-greedy policy. \n",
    "- `EXPLORE_FRAME`: frame number that epsilon starts to decay. \n",
    "- `GREEDY_FRAME`: frame number that epsilon ends to decay\n",
    "- `POLICY_UPDATE_FREQ`: frequency that policy net trains. \n",
    "- `TARGET_UPDATE_FREQ`: frequency that target net synchronized to policy net. \n",
    "- `LR`: learning rate. \n",
    "- `PER_EPS`: very small number that makes priority positive. \n",
    "- `PER_ALPHA`: between 0(uniform sampling) and 1(priority-based)\n",
    "- `PER_BETA_INIT`: between 0(no correction) and 1(correct distribution to uniform)\n",
    "- `PER_BETA_INC`: increasement of beta while training. \n",
    "- `MULTISTEP`: multi-step hyperparemeter. \n",
    "- `NOISE_SIGMA`: sigma for noisy network. \n",
    "- `DIST_N`: number of quantiles in distributional rl (QR-DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperParameter = namedtuple(\"HyperParameter\", (\n",
    "  \"NUM_EPISODES\",\n",
    "  \"TEST_EPISODES\",\n",
    "  \"TEST_FREQ\",\n",
    "  \"MEMORY_CAPACITY\",\n",
    "  \"BATCH_SIZE\",\n",
    "  \"GAMMA\",\n",
    "  \"EPS_START\",\n",
    "  \"EPS_END\",\n",
    "  \"EXPLORE_FRAME\",\n",
    "  \"GREEDY_FRAME\",\n",
    "  \"POLICY_UPDATE_FREQ\",\n",
    "  \"TARGET_UPDATE_FREQ\",\n",
    "  \"LR\",\n",
    "  \"PER_EPS\",\n",
    "  \"PER_ALPHA\",\n",
    "  \"PER_BETA_INIT\",\n",
    "  \"PER_BETA_INC\", \n",
    "  \"MULTISTEP\", \n",
    "  \"NOISE_SIGMA\", \n",
    "  \"DIST_N\"\n",
    "))\n",
    "\n",
    "# editable\n",
    "hp = HyperParameter(\n",
    "  NUM_EPISODES=1500,\n",
    "  TEST_EPISODES=5, \n",
    "  TEST_FREQ=100,\n",
    "  MEMORY_CAPACITY=50000,\n",
    "  BATCH_SIZE=64,\n",
    "  GAMMA=0.99,\n",
    "  EPS_START=0.9,\n",
    "  EPS_END=0.1,\n",
    "  EXPLORE_FRAME=1000,\n",
    "  GREEDY_FRAME=100000,\n",
    "  POLICY_UPDATE_FREQ=4,\n",
    "  TARGET_UPDATE_FREQ=1000,\n",
    "  LR=0.0001, \n",
    "  PER_EPS=0.00000001,\n",
    "  PER_ALPHA=0.6, \n",
    "  PER_BETA_INIT=0.4, \n",
    "  PER_BETA_INC=0.0001, \n",
    "  MULTISTEP=5, \n",
    "  NOISE_SIGMA=0.4, \n",
    "  DIST_N=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory\n",
    "\n",
    "Replay memory stores transitions that agent observed. Stored transitions can be used in training agents later. \n",
    "\n",
    "Transition consists of (`state`, `action`, `reward`, `next_state`, `done`). \n",
    "- `state`: current state. agent act at this state. \n",
    "- `action`: action acted at current state. \n",
    "- `reward`: reward from the action. (not equal to goal)\n",
    "- `next_state`: state after the action. zero state if terminated. \n",
    "- `done`: indicates whether the episode ended after the action. \n",
    "\n",
    "In this notebook, we supports two strategies to sample transitions. Both of the strategies can avoid data correlation problem. \n",
    "\n",
    "`UniformReplayMemory`: do uniform sampling - all transitions have same priority, so uniformly sample transitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"reward\", \"next_state\", \"done\"))\n",
    "\n",
    "class UniformReplayMemory(object):\n",
    "  \"\"\"\n",
    "  Replay Memory that uses uniform sampling strategy. \n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, capacity: int):\n",
    "    \"\"\"\n",
    "    Init memory which removes oldest data when overflows. \n",
    "    \"\"\"\n",
    "    self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "  def push(self, *args):\n",
    "    \"\"\"\n",
    "    Push new transition data into memory. \n",
    "    - Transition: (state, action, reward, next_state, done)\n",
    "    \"\"\"\n",
    "    \n",
    "    self.memory.append(Transition(*args))\n",
    "\n",
    "  def sample(self, batch_size: int):\n",
    "    \"\"\"\n",
    "    Return random samples according to uniform sampling. \n",
    "    Return form is Transition of samples. \n",
    "    ex) return.state is list of states. \n",
    "    \"\"\"\n",
    "    sample = random.sample(self.memory, batch_size)\n",
    "    return Transition(*zip(*sample))\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "    Return len of memory. \n",
    "    \"\"\"\n",
    "    return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PriorityReplayMemory`: prioritized experience sampling - each transition has its own priority, so do weighted sampling according to it. \n",
    "\n",
    "Priority of each transition is calculated as below. \n",
    "$$ P(i) = \\frac{p_i^\\alpha}{\\sum_k p_k^\\alpha} $$\n",
    "\n",
    "Importance Sampling weight is calculated to resolve bias problem.\n",
    "$$ w_i = \\left( \\frac{1}{N} \\frac{1}{P(i)} \\right)^\\beta $$\n",
    "\n",
    "After calculating the loss, priority updated. \n",
    "$$ p_i \\leftarrow \\text{calculated TD error} $$\n",
    "\n",
    "According to IS weights, parameters are updated according to $ \\Delta $ value. \n",
    "$$ \\Delta \\leftarrow \\Delta + w_i \\nabla_\\theta L_{\\theta,i} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object):\n",
    "  \"\"\"\n",
    "  Sumtree class for PER memory. \n",
    "  SumTree is a binary tree and each node stores sum of values of the child nodes. \n",
    "  Each leaf node shows data stored in the memory, and contains priority of the data. \n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, capacity: int):\n",
    "    \"\"\"\n",
    "    Initialize SumTree with given capacity. \n",
    "    - tree size is (2 * capacity - 1), which has (capacity) leaf nodes. \n",
    "    - data size is (capacity)\n",
    "    \"\"\"\n",
    "    self.capacity = capacity\n",
    "    \n",
    "    # define tree and data\n",
    "    self.tree = np.zeros(2 * self.capacity - 1)\n",
    "    self.data = np.zeros(self.capacity, dtype=object)\n",
    "    \n",
    "    # data count and pointer\n",
    "    self.data_cnt = 0\n",
    "    self.data_idx = 0\n",
    "    \n",
    "  def add(self, value: float, data: Transition):\n",
    "    \"\"\"\n",
    "    Add new data to SumTree.\n",
    "    \"\"\"\n",
    "    # update data array\n",
    "    self.data[self.data_idx] = data\n",
    "    \n",
    "    # update leaf and tree\n",
    "    tree_idx = self.data_idx + self.capacity - 1\n",
    "    self.update(tree_idx, value)\n",
    "    \n",
    "    # update data count and pointer\n",
    "    self.data_idx += 1\n",
    "    if self.data_idx >= self.capacity:\n",
    "      self.data_idx = 0\n",
    "    if self.data_cnt < self.capacity:\n",
    "      self.data_cnt += 1\n",
    "  \n",
    "  def update(self, tree_idx: int, value: float):\n",
    "    \"\"\"\n",
    "    Update value of leaf node and spread to its parent recursively. \n",
    "    \"\"\"\n",
    "    # update leaf\n",
    "    diff = value - self.tree[tree_idx]\n",
    "    self.tree[tree_idx] = value\n",
    "    \n",
    "    # update parent (until root)\n",
    "    while tree_idx > 0:\n",
    "      parent = (tree_idx - 1) // 2\n",
    "      self.tree[parent] += diff\n",
    "      tree_idx = parent\n",
    "  \n",
    "  def _retrieve(self, value: float):\n",
    "    \"\"\"\n",
    "    (Wrapped by get_leaf())\n",
    "    Find tree index that satisfies given cumulative sum. \n",
    "    Make sure that value is in [0, sum(priority)]\n",
    "    \"\"\"\n",
    "    tree_idx = 0\n",
    "    while True:\n",
    "      left_idx = tree_idx * 2 + 1\n",
    "      right_idx = left_idx + 1\n",
    "      \n",
    "      # if tree_idx is leaf, return it\n",
    "      if left_idx >= len(self.tree):\n",
    "        return tree_idx\n",
    "      \n",
    "      # choose left or right child\n",
    "      if value <= self.tree[left_idx]:\n",
    "        tree_idx = left_idx\n",
    "      else:\n",
    "        value -= self.tree[left_idx]\n",
    "        tree_idx = right_idx\n",
    "        \n",
    "  def get_leaf(self, value: float):\n",
    "    \"\"\"\n",
    "    Find tree index that satisfies given cumulative sum. \n",
    "    Make sure that value is in [0, sum(priority)]\n",
    "    - Return: (tree_idx, value, data) tuple\n",
    "    \"\"\"\n",
    "    tree_idx = self._retrieve(value)\n",
    "    return tree_idx, self.tree[tree_idx], self.data[tree_idx - self.capacity + 1]\n",
    "\n",
    "  def total_priority(self):\n",
    "    \"\"\"\n",
    "    Return total priority (same to root node's value)\n",
    "    \"\"\"\n",
    "    return self.tree[0]\n",
    "  \n",
    "class PriorityReplayMemory(object):\n",
    "  \"\"\"\n",
    "  Memory that supports Priority Experience Replay. \n",
    "  store (state, action, reward, next state, done) in memory \n",
    "  and use random samples by priority. \n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, capacity: int, eps: float, alpha: float, beta_init: float, beta_inc: float):\n",
    "    \"\"\"\n",
    "    Initialize PER memory\n",
    "    \n",
    "    - eps: very small value that makes all values positive. \n",
    "    - alpha: hyperparameter between 0(uniform) and 1(priority-based)\n",
    "    - beta: hyperparameter between 0(no correnction) and 1(correct to uniform-like)\n",
    "    \"\"\"\n",
    "    # hyperparameters\n",
    "    self.eps = eps\n",
    "    self.alpha = alpha\n",
    "    self.beta = beta_init\n",
    "    self.beta_inc = beta_inc\n",
    "    \n",
    "    # sumtree\n",
    "    self.capacity = capacity\n",
    "    self.sumtree = SumTree(self.capacity)\n",
    "    \n",
    "  def push(self, error: float, *args):\n",
    "    \"\"\"\n",
    "    Push new transition data into sumtree. \n",
    "    \"\"\"\n",
    "    # translate priority to weight value\n",
    "    priority = (error + self.eps) ** self.alpha\n",
    "    self.sumtree.add(priority, Transition(*args))\n",
    "    \n",
    "  def sample(self, batch_size: int):\n",
    "    \"\"\"\n",
    "    Sample datas by priorities. \n",
    "    - Return: (batch, tree_idxs, IS_weights)\n",
    "      - tree_idxs: tree indexes of transitions in sampled batch\n",
    "      - IS_weights: weights to calculate loss\n",
    "    \"\"\"\n",
    "    sample = []\n",
    "    tree_idxs = []\n",
    "    priorities = []\n",
    "    \n",
    "    segment = self.sumtree.total_priority() / batch_size\n",
    "    for i in range(batch_size):\n",
    "      left = segment * i\n",
    "      right = segment * (i + 1)\n",
    "      \n",
    "      # get random leaf in the segment\n",
    "      value = np.random.uniform(left, right)\n",
    "      tree_idx, priority, transition = self.sumtree.get_leaf(value)\n",
    "      \n",
    "      # add to list\n",
    "      sample.append(transition)\n",
    "      tree_idxs.append(tree_idx)\n",
    "      priorities.append(priority)\n",
    "    \n",
    "    # calc importance sampling weights (IS weights)\n",
    "    priorities = np.array(priorities) / self.sumtree.total_priority()\n",
    "    is_weights = np.power(self.sumtree.data_cnt * priorities, -self.beta)\n",
    "    is_weights /= is_weights.max()\n",
    "    \n",
    "    # update beta\n",
    "    if self.beta < 1.0:\n",
    "      self.beta = np.min([1., self.beta + self.beta_inc])\n",
    "    \n",
    "    return Transition(*zip(*sample)), tree_idxs, is_weights\n",
    "  \n",
    "  def update(self, tree_idx: int, error: float):\n",
    "    \"\"\"\n",
    "    Update existing priority to newly-calculated error. \n",
    "    \"\"\"\n",
    "    priority = (error + self.eps) ** self.alpha\n",
    "    self.sumtree.update(tree_idx, priority)\n",
    "    \n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "    Return length of memory\n",
    "    \"\"\"\n",
    "    return self.sumtree.data_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Network\n",
    "\n",
    "In this notebook, we use graphical environment that provides graphical observations and discrete actions. \n",
    "\n",
    "So, we use simple network containing few CNN filters and fully-connected layers. \n",
    "\n",
    "### Dueling Network\n",
    "Dueling network has feature extraction network, state head, and advantage head. \n",
    "\n",
    "Q(s, a) is calculated according to equation below. \n",
    "- V(s): scalar that shows state value. \n",
    "- Advantage(s, a): (action_num) vector that shows advantage of each action that others. \n",
    "$$ Q(s, a_i) = V(s) + Advantage(s, a_i) - E_j[Advantage(s, a_j)] $$\n",
    "\n",
    "### Noisy Network\n",
    "Noisy network add noise to network to perturb weight and bias in Linear layer. \n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "y = (\\mu^w + \\sigma^w \\odot \\epsilon^w)x + (\\mu^b + \\sigma^b \\odot \\epsilon^b) \\notag \\\\\n",
    "\n",
    "\\mu^w, \\sigma^w \\in R^{(p \\times q)}, \\mu^b, \\sigma^b \\in R^{p} \\text{: learnable parameters} \\notag \\\\\n",
    "\n",
    "\\epsilon^w \\in R^{(p \\times q)}, \\epsilon^b \\in R^{p} \\text{: random variables for each forward} \\notag \\\\\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "There are two randomize strategies. \n",
    "1. Independent Gaussian noise: randomize $ pq + p $ random variables independently. \n",
    "2. Factorised Gaussian noise: randomize $ p + q $ random variables to make two vectors (size of p and q), and calculate $ pq + p $ random variables. \n",
    "  $$ \n",
    "  \\begin{aligned}\n",
    "  \\epsilon_{i,j}^w &= f(\\epsilon_i)f(\\epsilon_j) \\\\ \n",
    "  \\epsilon_{i}^b &= f(\\epsilon_i) \\\\ \n",
    "  \\text{where } f(x) &= sgn(x) \\sqrt{|x|}\n",
    "  \\end{aligned}\n",
    "  $$\n",
    "\n",
    "### Distributional RL (QR-DQN)\n",
    "QR-DQN's output is (BATCH_SIZE, N, NUM_ACTIONS) tensor that shows supports for quantiles for each action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Linear):\n",
    "  \"\"\"\n",
    "  Linear layer with parameter perturbation. \n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, in_features: int, out_features: int):\n",
    "    \"\"\"\n",
    "    Define new linear layer with noise. \n",
    "    \"\"\"\n",
    "    super(NoisyLinear, self).__init__(in_features, out_features)\n",
    "\n",
    "    # init sigma for weight and bias\n",
    "    sigma = hp.NOISE_SIGMA / np.sqrt(in_features)\n",
    "    self.sigma_weight = nn.Parameter(torch.Tensor(out_features, in_features).fill_(sigma))\n",
    "    self.sigma_bias = nn.Parameter(torch.Tensor(out_features).fill_(sigma))\n",
    "    \n",
    "    # random variables\n",
    "    self.register_buffer(\"eps_in\", torch.zeros((1, in_features)))\n",
    "    self.register_buffer(\"eps_out\", torch.zeros((out_features, 1)))\n",
    "    self.register_buffer(\"eps_weight\", torch.zeros(self.sigma_weight.shape))\n",
    "    self.register_buffer(\"eps_bias\", torch.zeros(self.sigma_bias.shape))\n",
    "  \n",
    "  def f(self, x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    f(x) = sign(x) * sqrt(abs(x))\n",
    "    \"\"\"\n",
    "    return torch.sign(x) * torch.sqrt(torch.abs(x))\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Forward with randomized variables. \n",
    "    \"\"\"\n",
    "    # x shape is (B, in_features)\n",
    "    assert len(x.shape) == 2\n",
    "    \n",
    "    # generate random variables\n",
    "    self.eps_in = torch.randn(self.eps_in.shape, device=device)\n",
    "    self.eps_out = torch.randn(self.eps_out.shape, device=device)\n",
    "    \n",
    "    self.eps_weight = torch.mul(self.f(self.eps_out), self.f(self.eps_in))\n",
    "    self.eps_bias = self.eps_out.squeeze(1)\n",
    "    \n",
    "    # return with noisy weight and bias\n",
    "    weight = self.weight + self.sigma_weight * self.eps_weight\n",
    "    bias = self.bias + self.sigma_bias * self.eps_bias\n",
    "    return F.linear(x, weight, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "  \"\"\"\n",
    "  Q network that gets graphical input and discrete output. \n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(\n",
    "    self, \n",
    "    dim_observation: tuple, \n",
    "    n_actions: int, \n",
    "    dueling: bool, \n",
    "    noisy: bool, \n",
    "    distributional: bool\n",
    "  ):\n",
    "    \"\"\"\n",
    "    dim_observation input shape and n_actions output channels. \n",
    "    \"\"\"\n",
    "    super(QNetwork, self).__init__()\n",
    "    C, H, W = dim_observation\n",
    "    assert H == 84\n",
    "    assert W == 84\n",
    "\n",
    "    self.dueling = dueling\n",
    "    self.noisy = noisy\n",
    "    self.distributional = distributional\n",
    "    \n",
    "    linear = NoisyLinear if self.noisy else nn.Linear\n",
    "    \n",
    "    if self.dueling:\n",
    "      # dueling net: feature, state, and advantage\n",
    "      self.feature = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=C, out_channels=32, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten()\n",
    "      )\n",
    "      self.state = nn.Sequential(\n",
    "        linear(3136, 256), \n",
    "        nn.ReLU()\n",
    "      )\n",
    "      self.advantage = nn.Sequential(\n",
    "        linear(3136, 256), \n",
    "        nn.ReLU()\n",
    "      )\n",
    "      if self.distributional:\n",
    "        # state output: [BATCH_SIZE, N]\n",
    "        self.state += nn.Sequential(\n",
    "          linear(256, hp.DIST_N), \n",
    "          nn.Unflatten(1, (hp.DIST_N, 1))\n",
    "        )\n",
    "        # advantage output: [BATCH_SIZE, N, N_ACTIONS]\n",
    "        self.advantage += nn.Sequential(\n",
    "          linear(256, hp.DIST_N * n_actions), \n",
    "          nn.Unflatten(1, (hp.DIST_N, n_actions))\n",
    "        )\n",
    "      else:\n",
    "        # state output: [BATCH_SIZE, 1]\n",
    "        self.state += nn.Sequential(\n",
    "          linear(256, 1)\n",
    "        )\n",
    "        # advantage output: [BATCH_SIZE, N_ACTIONS]\n",
    "        self.advantage += nn.Sequential(\n",
    "          linear(256, n_actions)\n",
    "        )\n",
    "    else:\n",
    "      # q network\n",
    "      self.seqmodel = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=C, out_channels=32, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        linear(3136, 512),\n",
    "        nn.ReLU()\n",
    "      )\n",
    "      if self.distributional:\n",
    "        # output: [BATCH_SIZE, N, N_ACTIONS]\n",
    "        self.seqmodel += nn.Sequential(\n",
    "          linear(512, hp.DIST_N * n_actions), \n",
    "          nn.Unflatten(1, (hp.DIST_N, n_actions))\n",
    "        )\n",
    "      else:\n",
    "        # output: [BATCH_SIZE, N_ACTIONS]\n",
    "        self.seqmodel += nn.Sequential(\n",
    "          linear(512, n_actions)\n",
    "        )\n",
    "      \n",
    "  def forward(self, x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Forward.\n",
    "    \"\"\"\n",
    "    if self.dueling:\n",
    "      # get feature\n",
    "      feature = self.feature(x)\n",
    "\n",
    "      # get state value and advantage\n",
    "      state = self.state(feature)\n",
    "      advantage = self.advantage(feature)\n",
    "\n",
    "      # calc output and return\n",
    "      average = torch.mean(advantage, dim=-1, keepdim=True)\n",
    "      return state + advantage - average\n",
    "    else:\n",
    "      return self.seqmodel(x)\n",
    "    \n",
    "  def get_noise(self):\n",
    "    \"\"\"\n",
    "    If this is noisy network, returns scalar value that indicates noise. \n",
    "    |sigma_weight| ** 2 + |sigma_bias| ** 2\n",
    "    \"\"\"\n",
    "    noise = 0.0\n",
    "    for module in self.modules():\n",
    "      if isinstance(module, NoisyLinear):\n",
    "        noise += torch.sqrt(torch.sum(module.sigma_weight ** 2)).item()\n",
    "        noise += torch.sqrt(torch.sum(module.sigma_bias ** 2)).item()\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Utility Functions\n",
    "\n",
    "- `select_action()`: select agent's action in e-greedy policy. \n",
    "- `save_plot()`: plot epsilon, loss, average frames and score. \n",
    "- `save_model()`: store model, hyperparameters, and training info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy net and target net\n",
    "dim_observation = env.observation_space.shape\n",
    "n_actions = int(env.action_space.n)\n",
    "\n",
    "policy_net = QNetwork(\n",
    "  dim_observation, \n",
    "  n_actions, \n",
    "  dueling=op.DUELING, \n",
    "  noisy=op.NOISY, \n",
    "  distributional=op.DISTRIBUTIONAL\n",
    ").to(device)\n",
    "target_net = QNetwork(\n",
    "  dim_observation, \n",
    "  n_actions, \n",
    "  dueling=op.DUELING, \n",
    "  noisy=op.NOISY, \n",
    "  distributional=op.DISTRIBUTIONAL\n",
    ").to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# replay memory\n",
    "if op.PER:\n",
    "  # PER memory\n",
    "  memory = PriorityReplayMemory(\n",
    "    capacity=hp.MEMORY_CAPACITY, \n",
    "    eps=hp.PER_EPS, \n",
    "    alpha=hp.PER_ALPHA, \n",
    "    beta_init=hp.PER_BETA_INIT, \n",
    "    beta_inc=hp.PER_BETA_INC\n",
    "  )\n",
    "else:\n",
    "  # simple uniform memory\n",
    "  memory = UniformReplayMemory(hp.MEMORY_CAPACITY)\n",
    "  \n",
    "# quantiles for distributional RL\n",
    "if op.DISTRIBUTIONAL:\n",
    "  tau_hats = torch.tensor([[[(2 * i + 1) / (2 * hp.DIST_N)] for i in range(hp.DIST_N)]], device=device)\n",
    "  tau_hats = tau_hats.expand(hp.BATCH_SIZE, hp.DIST_N, 1)\n",
    "\n",
    "# save directory\n",
    "start_datetime = datetime.now()\n",
    "dirname = start_datetime.strftime(\"%Y%m%d-%H%M%S\")\n",
    "path = os.path.join(os.getcwd(), \"dqn\", dirname)\n",
    "\n",
    "# adamw optimizer\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=hp.LR, amsgrad=True)\n",
    "\n",
    "# training variables\n",
    "train_epsilons = []\n",
    "train_losses = []\n",
    "train_frames = []\n",
    "train_scores = []\n",
    "test_frames = []\n",
    "test_scores = []\n",
    "test_noises = []\n",
    "epsilon = hp.EPS_START\n",
    "epsilon_decay = (hp.EPS_START - hp.EPS_END) / (hp.GREEDY_FRAME - hp.EXPLORE_FRAME)\n",
    "steps = 0\n",
    "\n",
    "# if we use multi-step option, define transition buffer\n",
    "if op.MULTISTEP:\n",
    "  multistep_buffer = Transition(\n",
    "    state=deque(maxlen=hp.MULTISTEP),\n",
    "    action=deque(maxlen=hp.MULTISTEP),\n",
    "    reward=deque(maxlen=hp.MULTISTEP),\n",
    "    next_state=deque(maxlen=hp.MULTISTEP),\n",
    "    done=deque(maxlen=hp.MULTISTEP)\n",
    "  )\n",
    "\n",
    "def select_greedy(state: np.ndarray):\n",
    "  \"\"\"\n",
    "  Select agent's action by greedy policy. \n",
    "  \"\"\"\n",
    "  if op.DISTRIBUTIONAL:\n",
    "    with torch.no_grad():\n",
    "      return torch.argmax(torch.mean(policy_net(state), dim=1, keepdim=False), dim=1, keepdim=True)\n",
    "  else:\n",
    "    with torch.no_grad():\n",
    "      return torch.argmax(policy_net(state), dim=1, keepdim=True)\n",
    "\n",
    "def select_action(state: np.ndarray):\n",
    "  \"\"\"\n",
    "  Select agent's action by e-greedy policy. \n",
    "  \"\"\"\n",
    "  sample = random.random()\n",
    "  if sample > epsilon:\n",
    "    # greedy action\n",
    "    return select_greedy(state)\n",
    "  else:\n",
    "    # random action\n",
    "    return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "def save_plot():\n",
    "  \"\"\"\n",
    "  Plot loss, epsilon, average frames, and score and save the figures. \n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(16, 12))\n",
    "  plt.clf()\n",
    "  plt.ion()\n",
    "  \n",
    "  plt.subplot(2, 2, 1)\n",
    "  if op.NOISY:\n",
    "    plt.title(\"Noise\")\n",
    "    plt.xlabel(\"Test Iter\")\n",
    "    plt.ylabel(\"Noise\")\n",
    "    plt.plot(*zip(*test_noises), label=\"test\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "  else:\n",
    "    plt.title(\"Epsilon\")\n",
    "    plt.xlabel(\"Frames\")\n",
    "    plt.ylabel(\"Epsilon\")\n",
    "    plt.plot(*zip(*train_epsilons), label=\"train\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "  \n",
    "  plt.subplot(2, 2, 2)\n",
    "  plt.title(\"Loss\")\n",
    "  plt.xlabel(\"Learning Step\")\n",
    "  plt.ylabel(\"Loss\")\n",
    "  plt.plot(*zip(*train_losses), label=\"train\")\n",
    "  x, y = zip(*train_losses)\n",
    "  x = x[99:]\n",
    "  y = torch.mean(torch.tensor(y).unfold(0, 100, 1), dim=1)\n",
    "  plt.plot(x, y, label=\"train-avg100\")\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  \n",
    "  plt.subplot(2, 2, 3)\n",
    "  plt.title(\"# of Frames\")\n",
    "  plt.xlabel(\"Episode\")\n",
    "  plt.ylabel(\"# of Frames\")\n",
    "  plt.plot(*zip(*train_frames), label=\"train\")\n",
    "  plt.plot(*zip(*test_frames), label=\"test\")\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  \n",
    "  plt.subplot(2, 2, 4)\n",
    "  plt.title(\"Score\")\n",
    "  plt.xlabel(\"Episode\")\n",
    "  plt.ylabel(\"Score\")\n",
    "  plt.plot(*zip(*train_scores), label=\"train\")\n",
    "  plt.plot(*zip(*test_scores), label=\"test\")\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  \n",
    "  plt.ioff()\n",
    "  plt.savefig(os.path.join(path, \"plot.png\"))\n",
    "  \n",
    "  if is_ipython:\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "\n",
    "def save_model():\n",
    "  \"\"\"\n",
    "  Save model, hyperparameters, and training info.\n",
    "  \"\"\"\n",
    "  # save model\n",
    "  torch.save({\n",
    "    \"policy_net\": policy_net.state_dict(),\n",
    "    \"target_net\": target_net.state_dict()\n",
    "  }, os.path.join(path, \"model.pt\"))\n",
    "  \n",
    "  # save training options and hyperparameters\n",
    "  with open(os.path.join(path, \"option.json\"), \"w\") as w:\n",
    "    json.dump(dict([\n",
    "      (\"algorithm\", op._asdict()), \n",
    "      (\"hparam\", hp._asdict())\n",
    "    ]), w, indent=2)\n",
    "  \n",
    "  # save training info\n",
    "  with open(os.path.join(path, \"info.json\"), \"w\") as w:\n",
    "    json.dump(dict([\n",
    "      (\"env\", envname), \n",
    "      (\"test_frames\", test_frames), \n",
    "      (\"test_scores\", test_scores), \n",
    "      (\"test_noises\", test_noises), \n",
    "      (\"steps\", steps), \n",
    "      (\"training_time\", (datetime.now() - start_datetime).seconds)\n",
    "    ]), w, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Optimizing Function\n",
    "\n",
    "- `optimize_model()`: execute one step of optimizing. We use Huber loss in this notebook.\n",
    "  \n",
    "  $$ L(\\delta) = \\begin{cases} \n",
    "    \\frac{1}{2}\\delta^2 & \\text{for } |\\delta| \\leq 1 \\\\ \n",
    "    |\\delta| - \\frac{1}{2} & \\text{otherwise}\n",
    "  \\end{cases} $$\n",
    "  \n",
    "  - Original DQN learns according to equation below. \n",
    "  $$ \\delta = Q_{policy}(s_t, a_t) - (r_t + \\gamma max_a Q_{target}(s_{t+1}, a)) $$\n",
    "  \n",
    "\n",
    "  - DDQN learns according to equation below. \n",
    "  $$ \\delta = Q_{policy}(s_t, a_t) - (r_t + \\gamma Q_{target}(s_{t+1}, argmax_{a} Q_{policy}(s_{t+1}, a))) $$\n",
    "  \n",
    "  - Multi-step DQN learns according to equation below. \n",
    "    - n is hyperparameter for multi-step learning. \n",
    "    - $ G_{t:t+n} $ is cumulative rewards for n-steps. \n",
    "    - T is timestep when the episode ends. \n",
    "  $$ G_{t:t+n} = \\sum_{i=0}^{min(n-1, T)} \\gamma^{i} r_{t+i} $$\n",
    "  $$ \\delta = Q_{policy}(s_t, a_t) - (G_{t:t+n} + \\gamma^{n} max_a Q_{target}(s_{t+n}, a)) $$\n",
    "  \n",
    "  - Distributional DQN learns according to equation below. \n",
    "    \n",
    "      ||C51|QR-DQN(used)|\n",
    "      |-|-|-|\n",
    "      |Fixes|support|probability value|\n",
    "      |Learns|quantile fraction|support|\n",
    "    \n",
    "    1. Quantile\n",
    "        - quantile $ \\tau_i = \\frac{i}{N} \\text{ for } i = 1,···,N $ where N is number of quantiles. \n",
    "        - in this notebook, we use midpoint $ \\hat{\\tau_i} = \\frac{2(i - 1) + 1}{2N} \\text{ for } i = 1,···,N $ which is unique minimizer of Wasserstein Distnace. \n",
    "        - quantile regression $ Z_i = F_Z^-1(\\tau_i) $\n",
    "    2. Object\n",
    "        - objective of QR-DQN is minimizing Wasserstein Distance. In this notebook, we use p = 1. \n",
    "        $$ \n",
    "        \\begin{aligned}\n",
    "        &W_p(U, Y) = \\left( \\int_0^1 |F_Y^{-1}(w) - F_U^{-1}(w)|^p dw \\right)^{\\frac{1}{p}} \\\\\n",
    "        &\\text{U, Y: probability function} \\\\\n",
    "        &\\text{F: cumulative distribution function}\n",
    "        \\end{aligned}\n",
    "        $$\n",
    "        - Target is claculated as below. \n",
    "          $$ T\\theta_j \\leftarrow r + \\gamma * \\theta_j(x', a^*) \\quad \\forall j $$\n",
    "        - Loss is calculated as below. \n",
    "          1. loss increases where target distribution differs to prediction. \n",
    "          2. loss increases where right support has small value, or left support has large value. \n",
    "          $$ \n",
    "          \\begin{aligned}\n",
    "          &Loss = \\sum_{i=1}^N E_j\\left[ \\rho_{\\tau_i}(T \\theta_j - \\theta_i(x, a)) \\right] \\\\\n",
    "          &\\rho_\\tau(u) = \\begin{cases} \n",
    "            Huber(u)(1 - \\tau) \\quad &\\text{for } u < 0 \\\\ \n",
    "            Huber(u)(\\tau) &\\text{for } u \\geq 0\n",
    "          \\end{cases}\n",
    "          \\end{aligned}\n",
    "          $$\n",
    "    3. Network\n",
    "        - output of the network is (BATCH_SIZE, ACTION_NUM, N), which is quantile for each action. \n",
    "        - mean Q value is $ Z_\\theta(x, a) = \\frac{1}{N} \\sum_{i=1}^N \\delta_{\\theta_i}(x, a) $\n",
    "  \n",
    "  - Noisy DQN learns according to other options\n",
    "    - DQN or DDQN\n",
    "    - 1-step TD or n-step TD\n",
    "    - General or Distributional q value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "  \"\"\"\n",
    "  Optimize policy model. \n",
    "  \"\"\"\n",
    "  # if not enough data, quit\n",
    "  if len(memory) < hp.BATCH_SIZE:\n",
    "    return\n",
    "  \n",
    "  # sample batch\n",
    "  if op.PER:\n",
    "    # get from PER memory\n",
    "    batch, tree_idxs, is_weights = memory.sample(hp.BATCH_SIZE)\n",
    "  else:\n",
    "    # get from Simple memory\n",
    "    batch = memory.sample(hp.BATCH_SIZE)\n",
    "  \n",
    "  # get training data from the batch\n",
    "  state_batch = torch.cat(batch.state)\n",
    "  action_batch = torch.cat(batch.action)\n",
    "  reward_batch = torch.cat(batch.reward)\n",
    "  next_state_batch = torch.cat(batch.next_state)\n",
    "  done_batch = torch.cat(batch.done)\n",
    "  \n",
    "  if op.DISTRIBUTIONAL:\n",
    "    # calc Z(s_t, a)\n",
    "    current_z = policy_net(state_batch).gather(2, action_batch.unsqueeze(2).expand(hp.BATCH_SIZE, hp.DIST_N, 1))\n",
    "    \n",
    "    if op.DDQN:\n",
    "      # calc r_t + gamma * Z_target(s_t+1, argmax_a Q(s_t+1, a))\n",
    "      with torch.no_grad():\n",
    "        next_action = torch.argmax(torch.mean(policy_net(next_state_batch), dim=1, keepdim=False), dim=1, keepdim=True)\n",
    "        next_z = target_net(next_state_batch).gather(2, next_action.unsqueeze(2).expand(hp.BATCH_SIZE, hp.DIST_N, 1))\n",
    "    else:\n",
    "      # calc r_t + gamma * Z_target(s_t+1, argmax_a Q_target(s_t+1, a))\n",
    "      with torch.no_grad():\n",
    "        next_z = target_net(next_state_batch)\n",
    "        next_action = torch.argmax(torch.mean(next_z, dim=1, keepdim=False), dim=1, keepdim=True)\n",
    "        next_z = next_z.gather(2, next_action.unsqueeze(2).expand(hp.BATCH_SIZE, hp.DIST_N, 1))\n",
    "    expected_z = reward_batch.unsqueeze(2) + (hp.GAMMA ** hp.MULTISTEP if op.MULTISTEP else hp.GAMMA) \\\n",
    "      * (1.0 - done_batch.unsqueeze(2)) * next_z\n",
    "    expected_z = torch.transpose(expected_z, 1, 2)\n",
    "    \n",
    "    # calculate loss (dim=1: prediction dim, dim=2: target dim)\n",
    "    criterion = nn.HuberLoss(reduction=\"none\")\n",
    "    diff = expected_z - current_z\n",
    "    loss = criterion(current_z, expected_z)\n",
    "    loss *= torch.abs(tau_hats - (diff < 0).float())\n",
    "    loss = torch.mean(torch.sum(loss, dim=1, keepdim=False), dim=1, keepdim=True)\n",
    "    \n",
    "    if op.PER:\n",
    "      # calculate TD error\n",
    "      error = torch.mean(torch.sum(torch.abs(diff), dim=1, keepdim=False), dim=1, keepdim=True)\n",
    "      \n",
    "      # using calculated errors, update priorities of transitions\n",
    "      for idx in range(hp.BATCH_SIZE):\n",
    "        memory.update(tree_idxs[idx], error[idx].item())\n",
    "\n",
    "      # calc loss weighted by is_weight\n",
    "      is_weights = torch.tensor(is_weights, device=device).unsqueeze(1)\n",
    "      loss = torch.mul(loss, is_weights)\n",
    "      loss = torch.mean(loss)\n",
    "    else:\n",
    "      # get huber loss\n",
    "      loss = torch.mean(loss)\n",
    "  else:\n",
    "    # calc Q(s_t, a)\n",
    "    current_q = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    if op.DDQN:\n",
    "      # calc r_t + gamma * Q_target(s_t+1, argmax_a Q(s_t+1, a))\n",
    "      with torch.no_grad():\n",
    "        next_action = torch.argmax(policy_net(next_state_batch), dim=1, keepdim=True)\n",
    "        next_q = target_net(next_state_batch).gather(1, next_action)\n",
    "    else:\n",
    "      # calc r_t + gamma * max_a Q_target(s_t+1, a)\n",
    "      with torch.no_grad():\n",
    "        next_q = target_net(next_state_batch).max(1, keepdim=True).values\n",
    "    expected_q = reward_batch + (hp.GAMMA ** hp.MULTISTEP if op.MULTISTEP else hp.GAMMA) * (1.0 - done_batch) * next_q\n",
    "  \n",
    "    # calculate loss\n",
    "    if op.PER:\n",
    "      # calculate TD error\n",
    "      error = torch.abs(current_q - expected_q)\n",
    "      \n",
    "      # using calculated errors, update priorities of transitions\n",
    "      for idx in range(hp.BATCH_SIZE):\n",
    "        memory.update(tree_idxs[idx], error[idx].item())\n",
    "\n",
    "      # calc loss weighted by is_weight\n",
    "      criterion = nn.HuberLoss(reduction=\"none\")\n",
    "      is_weights = torch.tensor(is_weights, device=device).unsqueeze(1)\n",
    "      loss = criterion(current_q, expected_q)\n",
    "      loss = torch.mul(loss, is_weights)\n",
    "      loss = torch.mean(loss)\n",
    "    else:\n",
    "      # get huber loss\n",
    "      criterion = nn.HuberLoss()\n",
    "      loss = criterion(current_q, expected_q)\n",
    "  \n",
    "  loss_value = loss.item()\n",
    "  \n",
    "  # optimize model\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "  optimizer.step()\n",
    "  \n",
    "  return loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Testing Function\n",
    "\n",
    "- `test_model()`: run policy model in the environment with greedy policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "  \"\"\"\n",
    "  Test policy model and save the result in training variables. \n",
    "  \"\"\"\n",
    "  # testing variables\n",
    "  frames = []\n",
    "  scores = []\n",
    "  \n",
    "  # repeat for TEST_EPISODES episodes\n",
    "  for _ in range(1, hp.TEST_EPISODES + 1):\n",
    "    # initialize environment and state\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(np.array(state), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "    score = 0\n",
    "    \n",
    "    # start an episode\n",
    "    for frame in count():\n",
    "      # select greedy action\n",
    "      action = select_greedy(state)\n",
    "      \n",
    "      # act to next state\n",
    "      observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "      score += reward\n",
    "      done = terminated or truncated\n",
    "      \n",
    "      # update state\n",
    "      state = torch.tensor(np.array(observation), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "      # check end condition\n",
    "      if done:\n",
    "        frames.append(frame)\n",
    "        scores.append(score)\n",
    "        break\n",
    "      \n",
    "  # return average frames, scores, and noise\n",
    "  return np.mean(np.array(frames)), np.mean(np.array(scores)), (policy_net.get_noise() if op.NOISY else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In training, simulate agent in the environment to create transitions, and trains the agent using `optimize_model()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training directory\n",
    "os.makedirs(path)\n",
    "\n",
    "for episode in tqdm(range(1, hp.NUM_EPISODES + 1)):\n",
    "  # initialize environment and state\n",
    "  state, _ = env.reset()\n",
    "  state = torch.tensor(np.array(state), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "  score = 0\n",
    "  \n",
    "  # start an episode\n",
    "  for frame in count():\n",
    "    steps += 1\n",
    "    \n",
    "    if op.NOISY:\n",
    "      # noisy net with greedy policy\n",
    "      action = select_greedy(state)\n",
    "    else:\n",
    "      # select e-greedy action\n",
    "      action = select_action(state)\n",
    "    \n",
    "      # update epsilon\n",
    "      if steps < hp.EXPLORE_FRAME:\n",
    "        train_epsilons = [(1, hp.EPS_START), (steps, hp.EPS_START)]\n",
    "      elif steps < hp.GREEDY_FRAME:\n",
    "        epsilon -= epsilon_decay\n",
    "        train_epsilons = [(1, hp.EPS_START), (hp.EXPLORE_FRAME, hp.EPS_START), (steps, epsilon)]\n",
    "      else:\n",
    "        train_epsilons = [(1, hp.EPS_START), (hp.EXPLORE_FRAME, hp.EPS_START), (hp.GREEDY_FRAME, epsilon), (steps, epsilon)]\n",
    "    \n",
    "    # act to next state\n",
    "    observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "    score += reward\n",
    "    reward = torch.tensor([[reward]], device=device, dtype=torch.float32)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    # get next state\n",
    "    next_state = torch.tensor(np.array(observation), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # multi-step\n",
    "    if op.MULTISTEP:\n",
    "      # append transition to buffer\n",
    "      multistep_buffer.state.append(state)\n",
    "      multistep_buffer.action.append(action)\n",
    "      multistep_buffer.reward.append(reward)\n",
    "      multistep_buffer.next_state.append(next_state)\n",
    "      multistep_buffer.done.append(done)\n",
    "            \n",
    "      # if buffer full and not done, calculate n-step overall transition\n",
    "      available = len(multistep_buffer.state) == hp.MULTISTEP \\\n",
    "        and (True not in [multistep_buffer.done[i] for i in range(hp.MULTISTEP - 1)])\n",
    "      if available:\n",
    "        state_total = multistep_buffer.state[0]\n",
    "        action_total = multistep_buffer.action[0]\n",
    "        reward_total = multistep_buffer.reward\n",
    "        reward_total = sum([(hp.GAMMA ** i) * r for i, r in enumerate(reward_total)])\n",
    "        next_state_total = multistep_buffer.next_state[-1]\n",
    "        done_total = multistep_buffer.done[-1]\n",
    "      else:\n",
    "        state_total = None\n",
    "        action_total = None\n",
    "        reward_total = None\n",
    "        next_state_total = None\n",
    "        done_total = None  \n",
    "    else:\n",
    "      # no multistep: just use 1-td\n",
    "      state_total = state\n",
    "      action_total = action\n",
    "      reward_total = reward\n",
    "      next_state_total = next_state\n",
    "      done_total = done\n",
    "    \n",
    "    if state_total is not None:\n",
    "      # add transition to the memory\n",
    "      if op.PER:\n",
    "        if op.DISTRIBUTIONAL:\n",
    "          # calculate TD error\n",
    "          with torch.no_grad():\n",
    "            # calc Z(s_t, a)\n",
    "            current_z = policy_net(state_total).gather(2, action_total.unsqueeze(2).expand(1, hp.DIST_N, 1))\n",
    "            \n",
    "            if done_total:\n",
    "              expected_z = reward_total.unsqueeze(2)\n",
    "            else:\n",
    "              if op.DDQN:\n",
    "                # calc r_t + gamma * Z_target(s_t+1, argmax_a Q(s_t+1, a))\n",
    "                next_action = torch.argmax(torch.mean(policy_net(next_state_total), dim=1, keepdim=False), dim=1, keepdim=True)\n",
    "                next_z = target_net(next_state_total).gather(2, next_action.unsqueeze(2).expand(1, hp.DIST_N, 1))\n",
    "              else:\n",
    "                # calc r_t + gamma * Z_target(s_t+1, argmax_a Q_target(s_t+1, a))\n",
    "                next_z = target_net(next_state_total)\n",
    "                next_action = torch.argmax(torch.mean(next_z, dim=1, keepdim=False), dim=1, keepdim=True)\n",
    "                next_z = next_z.gather(2, next_action.unsqueeze(2).expand(1, hp.DIST_N, 1))\n",
    "              expected_z = reward_total.unsqueeze(2) + (hp.GAMMA ** hp.MULTISTEP if op.MULTISTEP else hp.GAMMA) * next_z\n",
    "              expected_z = torch.transpose(expected_z, 1, 2)\n",
    "            \n",
    "            # calculate TD error (dim=1: prediction dim, dim=2: target dim)\n",
    "            error = torch.mean(torch.sum(torch.abs(expected_z - current_z), dim=1, keepdim=False), dim=1, keepdim=True).item()\n",
    "        else:\n",
    "          # calculate TD error\n",
    "          with torch.no_grad():\n",
    "            # current Q_current\n",
    "            current_q = policy_net(state_total).gather(1, action_total)\n",
    "            \n",
    "            # calculate Q_expected = r + gamma * Q_next\n",
    "            if done_total:\n",
    "              expected_q = reward_total\n",
    "            else:\n",
    "              if op.DDQN:\n",
    "                next_action = torch.argmax(policy_net(next_state_total), dim=1, keepdim=True)\n",
    "                next_q = target_net(next_state_total).gather(1, next_action)\n",
    "              else:\n",
    "                next_q = target_net(next_state_total).max(1, keepdim=True).values\n",
    "              expected_q = reward_total + (hp.GAMMA ** hp.MULTISTEP if op.MULTISTEP else hp.GAMMA) * next_q\n",
    "            \n",
    "            # calculate td error\n",
    "            error = torch.abs(current_q - expected_q).item()\n",
    "          \n",
    "        # push transition to priority memory (with calculated priority)\n",
    "        done_tensor = torch.tensor([[1.0 if done_total else 0.0]], device=device)\n",
    "        memory.push(error, state_total, action_total, reward_total, next_state_total, done_tensor)\n",
    "      else:\n",
    "        # push transition to simple memory\n",
    "        done_tensor = torch.tensor([[1.0 if done_total else 0.0]], device=device)\n",
    "        memory.push(state_total, action_total, reward_total, next_state_total, done_tensor)\n",
    "      \n",
    "    # update state\n",
    "    state = next_state\n",
    "    \n",
    "    # if time to update policy net, optimize\n",
    "    if steps % hp.POLICY_UPDATE_FREQ == 0:\n",
    "      loss = optimize_model()\n",
    "      train_losses.append((steps // hp.POLICY_UPDATE_FREQ, loss))\n",
    "    \n",
    "    # if time to update target net, synchronize\n",
    "    if steps % hp.TARGET_UPDATE_FREQ == 0:\n",
    "      target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    # check end condition\n",
    "    if done:\n",
    "      train_frames.append((episode, frame))\n",
    "      train_scores.append((episode, score))\n",
    "      if episode % hp.TEST_FREQ == 0:\n",
    "        # add to training variables\n",
    "        mean_frame, mean_score, noise = test_model()\n",
    "        test_frames.append((episode, mean_frame))\n",
    "        test_scores.append((episode, mean_score))\n",
    "        if noise:\n",
    "          test_noises.append((episode, noise))\n",
    "        \n",
    "        # plot and save model\n",
    "        save_plot()\n",
    "        save_model()\n",
    "      break\n",
    "\n",
    "env.close()\n",
    "save_plot()\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "In this block, trained agent plays in the environment. We can see rendered environment played by the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(envname, render_mode=\"human\")\n",
    "#env = gym.make(envname)\n",
    "env = preprocess_env(env)\n",
    "\n",
    "scores = []\n",
    "\n",
    "# if you want to load from trained model, edit this (editable)\n",
    "load_dirname = None\n",
    "\n",
    "if load_dirname is not None:\n",
    "  # load models\n",
    "  path = os.path.join(os.getcwd(), \"dqn\", load_dirname)\n",
    "  checkpoint = torch.load(os.path.join(path, \"model.pt\"), map_location=device)\n",
    "  \n",
    "  policy_net.load_state_dict(checkpoint[\"policy_net\"])\n",
    "  target_net.load_state_dict(checkpoint[\"target_net\"])\n",
    "\n",
    "# repeat for TEST_EPISODES episodes\n",
    "for episode in range(1, hp.TEST_EPISODES + 1):\n",
    "  # initialize environment and state\n",
    "  state, _ = env.reset()\n",
    "  state = torch.tensor(np.array(state), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "  score = 0\n",
    "  \n",
    "  # start an episode\n",
    "  for _ in count():\n",
    "    # select greedy action\n",
    "    action = select_greedy(state)\n",
    "    \n",
    "    # act to next state\n",
    "    observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "    score += reward\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    # update state\n",
    "    state = torch.tensor(np.array(observation), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # check end condition\n",
    "    if done:\n",
    "      print(f\"Episode {episode}: {score}\")\n",
    "      scores.append(score)\n",
    "      break\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(f\"Average: {sum(scores) / hp.TEST_EPISODES}\")\n",
    "print(f\"Max: {max(scores)}\")\n",
    "print(f\"Min: {min(scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
