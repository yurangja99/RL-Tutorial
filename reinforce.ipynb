{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "Notebook that runs REINFORCE-based RL methods - REINFORCE, Actor-Critic, A2C, and A3C.\n",
    "The agent has been trained and tested on gymnasium environments. \n",
    "\n",
    "## REINFORCE-based methods\n",
    "- model-free\n",
    "- policy-based\n",
    "- on-policy (A3C: off-policy)\n",
    "\n",
    "||REINFORCE|Actor-Critic|A2C|A3C|\n",
    "|-|-|-|-|-|\n",
    "|bias|low|high|higher|high|\n",
    "|variance|high|low|lower|low|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "In this notebook, we use gymnasium environment. \n",
    "\n",
    "- CarRacing-v2 (discrete action space)\n",
    "- CarRacing-v2 (continuous action space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "from gymnasium.wrappers.gray_scale_observation import GrayScaleObservation\n",
    "from gymnasium.wrappers.frame_stack import FrameStack\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical, Normal\n",
    "\n",
    "def preprocess_env(env: gym.Env):\n",
    "  env = GrayScaleObservation(env, keep_dim=False)\n",
    "  env = FrameStack(env, num_stack=4)\n",
    "  return env\n",
    "\n",
    "# define environment\n",
    "envname = \"CarRacing-v2\"\n",
    "\n",
    "# continuous or discrete (editable)\n",
    "continuous = False\n",
    "env = gym.make(envname, continuous=continuous)\n",
    "env = preprocess_env(env)\n",
    "\n",
    "# plot settings\n",
    "is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "  from IPython import display\n",
    "\n",
    "# device setting (cpu or cuda!)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "- `ALGORITHM`: one of `[\"REINFORCE\", \"AC\", \"A2C\", \"A3C\"]`.\n",
    "- `NUM_EPISODES`: number of training episodes. \n",
    "- `TEST_EPISODES`: number of test episodes during training. \n",
    "- `TEST_FREQ`: testing frequency. \n",
    "- `GAMMA`: discount factor when calculating estimated goal. \n",
    "- `LR`: learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperParameter = namedtuple(\"HyperParameter\", (\n",
    "  \"ALGORITHM\",\n",
    "  \"NUM_EPISODES\",\n",
    "  \"TEST_EPISODES\",\n",
    "  \"TEST_FREQ\",\n",
    "  \"GAMMA\",\n",
    "  \"LR\"\n",
    "))\n",
    "\n",
    "# editable\n",
    "hp = HyperParameter(\n",
    "  ALGORITHM=\"REINFORCE\",\n",
    "  NUM_EPISODES=2000,\n",
    "  TEST_EPISODES=5,\n",
    "  TEST_FREQ=100,\n",
    "  GAMMA=0.99,\n",
    "  LR=0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor & Critic Network\n",
    "\n",
    "Network that receives 96x96 pixel information and returns (1) few discrete actions or (2) one continuous action, with calculated state value. \n",
    "We use simple network including CNN and fully-connected layers. \n",
    "\n",
    "We use value network for Actor-Critic, A2C, and A3C, so value part is also in the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "  \"\"\"\n",
    "  Policy network and value network with CNN and fully-connected layers. \n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(\n",
    "    self, \n",
    "    dim_observation: tuple, \n",
    "    action_space: Discrete | Box,\n",
    "    use_value_network: bool=False\n",
    "  ):\n",
    "    \"\"\"\n",
    "    n_observations input channels and n_actions output channels. \n",
    "    \"\"\"\n",
    "    super(ActorCriticNetwork, self).__init__()\n",
    "    C, H, W = dim_observation\n",
    "    assert H == 96\n",
    "    assert W == 96\n",
    "    \n",
    "    # feature extraction\n",
    "    self.seqmodel = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=C, out_channels=32, kernel_size=8, stride=4),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=2),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(4096, 512),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "    \n",
    "    # critic head\n",
    "    self.use_value_network = use_value_network\n",
    "    if self.use_value_network:\n",
    "      self.value = nn.Sequential(\n",
    "        nn.Linear(512, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 1)\n",
    "      )\n",
    "    \n",
    "    # actor head\n",
    "    # action can be discrete or continuous\n",
    "    self.discrete_action = isinstance(action_space, Discrete)\n",
    "    if self.discrete_action:\n",
    "      # discrete action: softmax output\n",
    "      self.action_prob = nn.Sequential(\n",
    "        nn.Linear(512, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, action_space.n),\n",
    "        nn.Softmax(dim=-1)\n",
    "      )\n",
    "    else:\n",
    "      # continuous action: vector of float\n",
    "      assert len(action_space.shape) == 1\n",
    "      self.mean = nn.Sequential(\n",
    "        nn.Linear(512, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, action_space.shape[0])\n",
    "      )\n",
    "      self.log_std = nn.Sequential(\n",
    "        nn.Linear(512, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, action_space.shape[0])\n",
    "      )\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Forward to get value and policy.\n",
    "    \"\"\"\n",
    "    x = self.seqmodel(x)\n",
    "    \n",
    "    # critic head\n",
    "    if self.use_value_network:\n",
    "      # value of the state\n",
    "      value = self.value(x)\n",
    "    else:\n",
    "      value = None\n",
    "    \n",
    "    # actor head\n",
    "    if self.discrete_action:\n",
    "      # discrete action\n",
    "      # output: ([,], [BATCH_SIZE, N_ACTIONS])\n",
    "      policy = Categorical(self.action_prob(x))\n",
    "    else:\n",
    "      # continuous action\n",
    "      # output: ([,], [BATCH_SIZE, ACTION_SPACE], [BATCH_SIZE, ACTION_SPACE])\n",
    "      mean = self.mean(x)\n",
    "      log_std = self.log_std(x)\n",
    "      log_std = torch.clamp(log_std, min=-20, max=2)\n",
    "      std = torch.exp(log_std)\n",
    "      policy = Normal(mean, std)\n",
    "    return value, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Utility Functions\n",
    "\n",
    "- `select_action`: select agent's action using policy network. \n",
    "- `save_plot`: plot objective func, average frames, and score. \n",
    "- `save_model`: store model, hyperparameters, and training info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm name validation\n",
    "assert hp.ALGORITHM in [\"REINFORCE\", \"AC\", \"A2C\", \"A3C\"]\n",
    "\n",
    "# policy network\n",
    "dim_observation = env.observation_space.shape\n",
    "action_space = env.action_space\n",
    "discrete_action = isinstance(env.action_space, Discrete)\n",
    "if not discrete_action:\n",
    "  action_low = torch.tensor(action_space.low, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "  action_high = torch.tensor(action_space.high, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "policy_net = ActorCriticNetwork(dim_observation, action_space, False).to(device)\n",
    "\n",
    "# adamw optimizer\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=hp.LR, amsgrad=True)\n",
    "\n",
    "# save directory\n",
    "start_datetime = datetime.now()\n",
    "dirname = start_datetime.strftime(\"%Y%m%d-%H%M%S\")\n",
    "path = os.path.join(os.getcwd(), \"reinforce_based\", dirname)\n",
    "\n",
    "# training variables\n",
    "train_objectives = []\n",
    "train_losses = []\n",
    "train_frames = []\n",
    "train_scores = []\n",
    "test_frames = []\n",
    "test_scores = []\n",
    "steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state: torch.Tensor):\n",
    "  \"\"\"\n",
    "  Select agent's action using policy network. \n",
    "  Don't use torch.no_grad() because REINFORCE is on-policy. \n",
    "  \"\"\"\n",
    "  assert state.shape[0] == 1\n",
    "  \n",
    "  # get value and policy\n",
    "  _, policy = policy_net(state)\n",
    "  # sample action\n",
    "  sample = policy.sample()\n",
    "  log_prob = policy.log_prob(sample)\n",
    "  \n",
    "  if discrete_action:\n",
    "    # select action\n",
    "    action = sample\n",
    "    # return with log probabilities\n",
    "    return action.item(), log_prob\n",
    "  else:\n",
    "    # get action: squeeze between low and high\n",
    "    # squeeze into [-1, 1]\n",
    "    # high * 0.5 * (1 + val) + low * 0.5 * (1 - val)\n",
    "    action = torch.tanh(sample)\n",
    "    action = action_low * 0.5 * (1.0 - action) + action_high * 0.5 * (1.0 + action)\n",
    "    # return with log probabilities\n",
    "    return action[0].cpu().numpy(), torch.sum(log_prob, dim=-1, keepdim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot():\n",
    "  \"\"\"\n",
    "  Plot objective func, losses, average frames, and scores and save the figures. \n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(16, 12))\n",
    "  plt.clf()\n",
    "  plt.ion()\n",
    "\n",
    "  plt.subplot(2, 2, 1)\n",
    "  plt.title(\"J(θ)\")\n",
    "  plt.xlabel(\"Episode\")\n",
    "  plt.ylabel(\"J(θ)\")\n",
    "  plt.plot(*zip(*train_objectives), label=\"train\")\n",
    "  if len(train_objectives) >= 100:\n",
    "    x, y = zip(*train_objectives)\n",
    "    y = torch.mean(torch.tensor(y).unfold(0, 100, 1), dim=1)\n",
    "    plt.plot(x[99:], y, label=\"train-avg100\")\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  \n",
    "  plt.subplot(2, 2, 2)\n",
    "  plt.title(\"Loss\")\n",
    "  plt.xlabel(\"Learning Step\")\n",
    "  plt.ylabel(\"Loss\")\n",
    "  plt.plot(*zip(*train_losses), label=\"train\")\n",
    "  if len(train_losses) >= 100:\n",
    "    x, y = zip(*train_losses)\n",
    "    y = torch.mean(torch.tensor(y).unfold(0, 100, 1), dim=1)\n",
    "    plt.plot(x[99:], y, label=\"train-avg100\")\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  \n",
    "  plt.subplot(2, 2, 3)\n",
    "  plt.title(\"# of Frames\")\n",
    "  plt.xlabel(\"Episode\")\n",
    "  plt.ylabel(\"# of Frames\")\n",
    "  plt.plot(*zip(*train_frames), label=\"train\")\n",
    "  plt.plot(*zip(*test_frames), label=\"test\")\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  \n",
    "  plt.subplot(2, 2, 4)\n",
    "  plt.title(\"Score\")\n",
    "  plt.xlabel(\"Episode\")\n",
    "  plt.ylabel(\"Score\")\n",
    "  plt.plot(*zip(*train_scores), label=\"train\")\n",
    "  plt.plot(*zip(*test_scores), label=\"test\")\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  \n",
    "  plt.ioff()\n",
    "  plt.savefig(os.path.join(path, \"plot.png\"))\n",
    "  \n",
    "  if is_ipython:\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "  \"\"\"\n",
    "  Save model, hyperparameters, and training info.\n",
    "  \"\"\"\n",
    "  # save model\n",
    "  torch.save({\n",
    "    \"policy_net\": policy_net.state_dict()\n",
    "  }, os.path.join(path, \"model.pt\"))\n",
    "  \n",
    "  # save hyperparameters\n",
    "  with open(os.path.join(path, \"hparam.json\"), \"w\") as w:\n",
    "    json.dump(hp._asdict(), w, indent=2)\n",
    "  \n",
    "  # save training info\n",
    "  with open(os.path.join(path, \"info.json\"), \"w\") as w:\n",
    "    json.dump(dict([\n",
    "      (\"env\", envname), \n",
    "      (\"test_frames\", test_frames), \n",
    "      (\"test_scores\", test_scores), \n",
    "      (\"steps\", steps), \n",
    "      (\"training_time\", (datetime.now() - start_datetime).seconds)\n",
    "    ]), w, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Testing Function\n",
    "\n",
    "- `test_model`: run policy model in the environment with trained policy network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "  \"\"\"\n",
    "  Test policy model and return the result in training variables. \n",
    "  \"\"\"\n",
    "  # testing variables\n",
    "  frames = []\n",
    "  scores = []\n",
    "  \n",
    "  # repeat for TEST_EPISODES episodes\n",
    "  for _ in range(1, hp.TEST_EPISODES + 1):\n",
    "    # initialize environment and state\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(np.array(state), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "    score = 0\n",
    "    \n",
    "    # start an episode\n",
    "    for frame in count():\n",
    "      # select action\n",
    "      with torch.no_grad():\n",
    "        action, _ = select_action(state)\n",
    "      \n",
    "      # act to next state\n",
    "      observation, reward, terminated, truncated, _ = env.step(action)\n",
    "      score += reward\n",
    "      done = terminated or truncated\n",
    "      \n",
    "      # update state\n",
    "      state = torch.tensor(np.array(observation), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "      # check end condition\n",
    "      if done:\n",
    "        frames.append(frame)\n",
    "        scores.append(score)\n",
    "        break\n",
    "      \n",
    "  # add to training variables\n",
    "  return np.mean(np.array(frames)), np.mean(np.array(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In training, simulate agent in the environment to create trajectory, and trains policy network. \n",
    "\n",
    "Object of REINFORCE algorithm is maximizing objective function, which is defined as below. \n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "J_i(\\theta_i) = E_{s0 \\sim p0} \\left[ v_{\\pi_{\\theta_i}}(s_0) \\right] = E_{\\tau \\sim \\pi_\\theta} \\left[ G(\\tau) \\right] \\notag \\\\\n",
    "\\text{where } \\tau = S_0, A_0, R_0, S_1, ..., S_{T-1}, A_{T-1}, R_{T-1}, S_T \\notag \\\\\n",
    "\\text{where } G(\\tau) = R_0 + \\gamma R_1 + ... + \\gamma^{T-1} R_{T-1} \\notag\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "We can get objective function using some mathematical tricks. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\theta J(\\theta) &= \\nabla_\\theta E_{\\tau \\sim \\pi_\\theta} \\left[ G(\\tau) \\right] \\\\ \n",
    "&= E_{\\tau \\sim \\pi_\\theta} \\left[ \\nabla_\\theta \\ln p(\\tau|\\pi_\\theta) G(\\tau) \\right] \\\\ \n",
    "&= E_{\\tau \\sim \\pi_\\theta} \\left[ G(\\tau) \\sum_{t=0}^T \\nabla_\\theta \\ln \\pi(A_t|S_t;\\pi_\\theta) \\right] \\\\ \n",
    "&= E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\gamma^t R_t \\sum_{t=0}^T \\nabla_\\theta \\ln \\pi(A_t|S_t;\\pi_\\theta) \\right] \\\\\n",
    "&= E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\left( \\left( \\sum_{k=t}^{T-1} \\gamma^k R_k \\right) \\nabla_\\theta \\ln \\pi(A_t|S_t;\\pi_\\theta) \\right) \\right] \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The last equation makes sense because the following can be shown: $ E_{\\tau \\sim \\pi_\\theta} \\left[\\gamma^k R_k \\nabla_\\theta \\ln \\pi(A_t | S_t ; \\pi_\\theta) \\right] = 0 \\text{ for } k < t $\n",
    "\n",
    "In this point, we approximate $ \\gamma $-included term to prevent reward vanishing. \n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\sum_{k=t}^{T-1} \\gamma^k R_k \n",
    "= \\gamma^t \\sum_{k=t}^{T-1}  \\gamma^{k-t} R_k \n",
    "= \\gamma^t G_t\n",
    "\\cong G_t \\notag \\\\\n",
    "\\nabla_\\theta J(\\theta) \n",
    "\\cong E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T G_t \\nabla_\\theta \\ln \\pi(A_t|S_t;\\pi_\\theta) \\right] \\notag\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "So, we update parameters as below. \n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^T G_t \\nabla_\\theta \\ln \\pi(A_t|S_t;\\pi_\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training directory\n",
    "os.makedirs(path)\n",
    "\n",
    "for episode in tqdm(range(1, hp.NUM_EPISODES + 1)):\n",
    "  # initialize environment and state\n",
    "  state, _ = env.reset()\n",
    "  state = torch.tensor(np.array(state), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "  score = 0\n",
    "  reward_history = [] # list of scalars\n",
    "  log_prob_history = [] # list of (1,) tensors\n",
    "  \n",
    "  # start an episode\n",
    "  for frame in count():\n",
    "    # select action\n",
    "    action, log_prob = select_action(state)\n",
    "    steps += 1\n",
    "    \n",
    "    # act to next state\n",
    "    observation, reward, terminated, truncated, _ = env.step(action)\n",
    "    score += reward\n",
    "    reward_history.append(reward)\n",
    "    log_prob_history.append(log_prob)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    # update state to next state\n",
    "    state = torch.tensor(np.array(observation), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "    \n",
    "    # check end condition\n",
    "    if done:\n",
    "      train_scores.append((episode, score))\n",
    "      train_frames.append((episode, frame))\n",
    "      break\n",
    "  \n",
    "  # get list of G_t\n",
    "  g_history = []\n",
    "  r = 0\n",
    "  for i in range(len(reward_history) - 1, -1, -1):\n",
    "    r = reward_history[i] + r * hp.GAMMA\n",
    "    g_history.insert(0, r)\n",
    "  g_history = torch.tensor(g_history, device=device)\n",
    "  \n",
    "  # normalize g_history\n",
    "  train_objectives.append((episode, g_history[0].item()))\n",
    "  g_history = (g_history - torch.mean(g_history)) / torch.std(g_history)\n",
    "  \n",
    "  # get list of log probs of actions\n",
    "  log_prob_history = torch.cat(log_prob_history).to(device=device)\n",
    "  \n",
    "  # gradient ascent\n",
    "  loss = -torch.sum(torch.mul(g_history, log_prob_history))\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  \n",
    "  if episode % hp.TEST_FREQ == 0:\n",
    "    mean_frame, mean_score = test_model()\n",
    "    test_frames.append((episode, mean_frame))\n",
    "    test_scores.append((episode, mean_score))\n",
    "    save_plot()\n",
    "    save_model()\n",
    "\n",
    "save_plot()\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "In this block, trained agent plays in the environment. We can see rendered environment played by the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(envname, render_mode=\"human\", continuous=continuous)\n",
    "env = preprocess_env(env)\n",
    "\n",
    "scores = []\n",
    "\n",
    "# if you want to load from trained model, edit this (editable)\n",
    "load_dirname = None\n",
    "\n",
    "if load_dirname is not None:\n",
    "  # load models\n",
    "  path = os.path.join(os.getcwd(), \"reinforce_based\", load_dirname)\n",
    "  checkpoint = torch.load(os.path.join(path, \"model.pt\"), map_location=device)\n",
    "  \n",
    "  policy_net.load_state_dict(checkpoint[\"policy_net\"])\n",
    "\n",
    "# repeat for TEST_EPISODES episodes\n",
    "for episode in range(1, hp.TEST_EPISODES + 1):\n",
    "  # initialize environment and state\n",
    "  state, _ = env.reset()\n",
    "  state = torch.tensor(np.array(state), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "  score = 0\n",
    "  \n",
    "  # start an episode\n",
    "  for _ in count():\n",
    "    # select greedy action\n",
    "    with torch.no_grad():\n",
    "      action, _ = select_action(state)\n",
    "    \n",
    "    # act to next state\n",
    "    observation, reward, terminated, truncated, _ = env.step(action)\n",
    "    score += reward\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    # update state\n",
    "    state = torch.tensor(np.array(observation), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # check end condition\n",
    "    if done:\n",
    "      print(f\"Episode {episode}: {score}\")\n",
    "      scores.append(score)\n",
    "      break\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(f\"Average: {sum(scores) / hp.TEST_EPISODES}\")\n",
    "print(f\"Max: {max(scores)}\")\n",
    "print(f\"Min: {min(scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
