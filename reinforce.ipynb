{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "Notebook that runs REINFORCE-based RL methods - REINFORCE, Actor-Critic, A2C, and A3C.\n",
    "The agent has been trained and tested on gymnasium environments. \n",
    "\n",
    "## REINFORCE-based methods\n",
    "- model-free\n",
    "- policy-based\n",
    "- on-policy (A3C: off-policy)\n",
    "\n",
    "||REINFORCE|Actor-Critic|A2C|A3C|\n",
    "|-|-|-|-|-|\n",
    "|bias|low|high|higher|high|\n",
    "|variance|high|low|lower|low|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "In this notebook, we use gymnasium environment. \n",
    "\n",
    "- CarRacing-v2 (discrete action space)\n",
    "- CarRacing-v2 (continuous action space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q numpy\n",
    "%pip install -q tqdm\n",
    "%pip install -q matplotlib\n",
    "%pip install -q torch\n",
    "%pip install -q swig\n",
    "%pip install -q gymnasium\n",
    "%pip install -q gymnasium[box2d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cuda\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "from gymnasium.wrappers.gray_scale_observation import GrayScaleObservation\n",
    "from gymnasium.wrappers.frame_stack import FrameStack\n",
    "from tqdm import tqdm\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import HuberLoss\n",
    "from torch.distributions import Categorical, Normal\n",
    "\n",
    "def preprocess_env(env: gym.Env):\n",
    "  env = GrayScaleObservation(env, keep_dim=False)\n",
    "  env = FrameStack(env, num_stack=4)\n",
    "  return env\n",
    "\n",
    "# define environment\n",
    "envname = \"CarRacing-v2\"\n",
    "\n",
    "# continuous or discrete (editable)\n",
    "continuous = False\n",
    "env = gym.make(envname, continuous=continuous)\n",
    "env = preprocess_env(env)\n",
    "\n",
    "# plot settings\n",
    "is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "  from IPython import display\n",
    "\n",
    "# device setting (cpu or cuda!)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "- `ALGORITHM`: one of `[\"REINFORCE\", \"AC\", \"A2C\", \"A3C\"]`.\n",
    "- `NUM_EPISODES`: number of training episodes. \n",
    "- `TEST_EPISODES`: number of test episodes during training. \n",
    "- `TEST_FREQ`: testing frequency. \n",
    "- `GAMMA`: discount factor when calculating estimated goal. \n",
    "- `LR`: learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperParameter = namedtuple(\"HyperParameter\", (\n",
    "  \"ALGORITHM\",\n",
    "  \"NUM_EPISODES\",\n",
    "  \"TEST_EPISODES\",\n",
    "  \"TEST_FREQ\",\n",
    "  \"GAMMA\",\n",
    "  \"LR\"\n",
    "))\n",
    "\n",
    "# editable\n",
    "hp = HyperParameter(\n",
    "  ALGORITHM=\"REINFORCE\",\n",
    "  NUM_EPISODES=2000,\n",
    "  TEST_EPISODES=5,\n",
    "  TEST_FREQ=100,\n",
    "  GAMMA=0.99,\n",
    "  LR=0.0005\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor & Critic Network\n",
    "\n",
    "Network that receives 96x96 pixel information and returns (1) few discrete actions or (2) one continuous action, with calculated state value. \n",
    "We use simple network including CNN and fully-connected layers. \n",
    "\n",
    "We use value network for Actor-Critic, A2C, and A3C, so value part is also in the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticNetwork(nn.Module):\n",
    "  \"\"\"\n",
    "  Policy network and value network with CNN and fully-connected layers. \n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(\n",
    "    self, \n",
    "    dim_observation: tuple, \n",
    "    action_space: Discrete | Box,\n",
    "    use_state_value_network: bool=False,\n",
    "    use_action_value_network: bool=False\n",
    "  ):\n",
    "    \"\"\"\n",
    "    n_observations input channels and n_actions output channels. \n",
    "\n",
    "    Args:\n",
    "        dim_observation (tuple): environment's observation dimension\n",
    "        action_space (Discrete | Box): environment's action space, which is Discrete or Box\n",
    "        use_state_value_network (bool, optional): whether we use state value network or not. True for A2C and A3C. \n",
    "        use_action_value_network (bool, optional): whether we use action value network or not. True for AC.\n",
    "    \"\"\"\n",
    "    super(ActorCriticNetwork, self).__init__()\n",
    "    C, H, W = dim_observation\n",
    "    assert H == 96\n",
    "    assert W == 96\n",
    "    \n",
    "    # feature extraction\n",
    "    self.seq_model = nn.Sequential(\n",
    "      nn.Conv2d(in_channels=C, out_channels=32, kernel_size=8, stride=4),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=2),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(4096, 512),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "    \n",
    "    # actor head\n",
    "    # action can be discrete or continuous\n",
    "    self.is_discrete_action = isinstance(action_space, Discrete)\n",
    "    if self.is_discrete_action:\n",
    "      # discrete action: softmax output\n",
    "      self.policy_prob_model = nn.Sequential(\n",
    "        nn.Linear(512, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, action_space.n),\n",
    "        nn.Softmax(dim=-1)\n",
    "      )\n",
    "    else:\n",
    "      # continuous action: vector of float\n",
    "      assert len(action_space.shape) == 1\n",
    "      self.policy_mean_model = nn.Sequential(\n",
    "        nn.Linear(512, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, action_space.shape[0])\n",
    "      )\n",
    "      self.policy_log_std_model = nn.Sequential(\n",
    "        nn.Linear(512, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, action_space.shape[0])\n",
    "      )\n",
    "    \n",
    "    # critic head - state value\n",
    "    self.use_state_value_network = use_state_value_network\n",
    "    if self.use_state_value_network:\n",
    "      self.state_value_model = nn.Sequential(\n",
    "        nn.Linear(512, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 1)\n",
    "      )\n",
    "    \n",
    "    # critic head - action value\n",
    "    self.use_action_value_network = use_action_value_network\n",
    "    if self.use_action_value_network:\n",
    "      # action value only available for discrete action\n",
    "      assert self.is_discrete_action\n",
    "      self.action_value_model = nn.Sequential(\n",
    "        nn.Linear(512, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, action_space.n)\n",
    "      )\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Forward to get value and policy.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): input tensor\n",
    "\n",
    "    Returns:\n",
    "        state_value (torch.Tensor): state value\n",
    "        action_value (torch.Tensor): state-action value\n",
    "        policy (Categorical | Normal): distribution of action probabilities\n",
    "    \"\"\"\n",
    "    x = self.seq_model(x)\n",
    "    \n",
    "    # actor head\n",
    "    if self.is_discrete_action:\n",
    "      # discrete action\n",
    "      policy = Categorical(self.policy_prob_model(x))\n",
    "    else:\n",
    "      # continuous action\n",
    "      mean = self.policy_mean_model(x)\n",
    "      log_std = self.policy_log_std_model(x)\n",
    "      log_std = torch.clamp(log_std, min=-20, max=2)\n",
    "      std = torch.exp(log_std)\n",
    "      policy = Normal(mean, std)\n",
    "    \n",
    "    # critic head - state value\n",
    "    if self.use_state_value_network:\n",
    "      # state value\n",
    "      state_value = self.state_value_model(x)\n",
    "    else:\n",
    "      state_value = None\n",
    "      \n",
    "    # critic head - action value\n",
    "    if self.use_action_value_network:\n",
    "      # action value\n",
    "      action_value = self.action_value_model(x)\n",
    "    else:\n",
    "      action_value = None\n",
    "    \n",
    "    return state_value, action_value, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Utility Functions\n",
    "\n",
    "- `select_action`: select agent's action using policy network. \n",
    "- `save_plot`: plot objective func, average frames, and score. \n",
    "- `save_model`: store model, hyperparameters, and training info. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm name validation\n",
    "assert hp.ALGORITHM in [\"REINFORCE\", \"AC\", \"A2C\", \"A3C\"]\n",
    "\n",
    "# policy network\n",
    "dim_observation = env.observation_space.shape\n",
    "action_space = env.action_space\n",
    "is_discrete_action = isinstance(env.action_space, Discrete)\n",
    "if not is_discrete_action:\n",
    "  action_low = torch.tensor(action_space.low, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "  action_high = torch.tensor(action_space.high, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "policy_net = ActorCriticNetwork(\n",
    "  dim_observation, \n",
    "  action_space, \n",
    "  hp.ALGORITHM in [\"A2C\", \"A3C\"],\n",
    "  hp.ALGORITHM == \"AC\"\n",
    ").to(device)\n",
    "\n",
    "# adamw optimizer\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=hp.LR, amsgrad=True)\n",
    "\n",
    "# save directory\n",
    "start_datetime = datetime.now()\n",
    "dirname = start_datetime.strftime(\"%Y%m%d-%H%M%S\")\n",
    "path = os.path.join(os.getcwd(), \"reinforce_based\", dirname)\n",
    "\n",
    "# training variables\n",
    "train_objectives = []\n",
    "train_actor_losses = []\n",
    "train_critic_losses = []\n",
    "train_frames = []\n",
    "train_scores = []\n",
    "test_frames = []\n",
    "test_scores = []\n",
    "steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state: torch.Tensor):\n",
    "  \"\"\"\n",
    "  Select agent's action using policy network. \n",
    "  Don't use torch.no_grad() because REINFORCE is on-policy. \n",
    "  \"\"\"\n",
    "  assert state.shape[0] == 1\n",
    "  \n",
    "  # get value and policy\n",
    "  state_value, action_value, policy = policy_net(state)\n",
    "  # sample action\n",
    "  sample = policy.sample()\n",
    "  log_prob = policy.log_prob(sample)\n",
    "  \n",
    "  if is_discrete_action:\n",
    "    # select action\n",
    "    action = sample\n",
    "    # return with log probabilities\n",
    "    return state_value, action_value, action.item(), log_prob\n",
    "  else:\n",
    "    # get action: squeeze between low and high\n",
    "    # squeeze into [-1, 1]\n",
    "    # high * 0.5 * (1 + val) + low * 0.5 * (1 - val)\n",
    "    action = torch.tanh(sample)\n",
    "    action = action_low * 0.5 * (1.0 - action) + action_high * 0.5 * (1.0 + action)\n",
    "    # return with log probabilities\n",
    "    return state_value, action_value, action[0].cpu().numpy(), torch.sum(log_prob, dim=-1, keepdim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot():\n",
    "  \"\"\"\n",
    "  Plot objective func, losses, average frames, and scores and save the figures. \n",
    "  \"\"\"\n",
    "  plt.figure(figsize=(16, 12))\n",
    "  plt.clf()\n",
    "  plt.ion()\n",
    "\n",
    "  plt.subplot(2, 2, 1)\n",
    "  plt.title(\"J(θ)\")\n",
    "  plt.xlabel(\"Episode\")\n",
    "  plt.ylabel(\"J(θ)\")\n",
    "  plt.plot(*zip(*train_objectives), label=\"train\")\n",
    "  if len(train_objectives) >= 100:\n",
    "    x, y = zip(*train_objectives)\n",
    "    y = torch.mean(torch.tensor(y).unfold(0, 100, 1), dim=1)\n",
    "    plt.plot(x[99:], y, label=\"train-avg100\")\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  \n",
    "  plt.subplot(2, 2, 2)\n",
    "  plt.title(\"Loss\")\n",
    "  plt.xlabel(\"Learning Step\")\n",
    "  plt.ylabel(\"Loss\")\n",
    "  if len(train_actor_losses) >= 100:\n",
    "    x, y = zip(*train_actor_losses)\n",
    "    y = torch.mean(torch.tensor(y).unfold(0, 100, 1), dim=1)\n",
    "    plt.plot(x[99:], y, label=\"train-actor-avg100\")\n",
    "  if len(train_critic_losses) >= 100:\n",
    "    x, y = zip(*train_critic_losses)\n",
    "    y = torch.mean(torch.tensor(y).unfold(0, 100, 1), dim=1)\n",
    "    plt.plot(x[99:], y, label=\"train-critic-avg100\")\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  \n",
    "  plt.subplot(2, 2, 3)\n",
    "  plt.title(\"# of Frames\")\n",
    "  plt.xlabel(\"Episode\")\n",
    "  plt.ylabel(\"# of Frames\")\n",
    "  plt.plot(*zip(*train_frames), label=\"train\")\n",
    "  plt.plot(*zip(*test_frames), label=\"test\")\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  \n",
    "  plt.subplot(2, 2, 4)\n",
    "  plt.title(\"Score\")\n",
    "  plt.xlabel(\"Episode\")\n",
    "  plt.ylabel(\"Score\")\n",
    "  plt.plot(*zip(*train_scores), label=\"train\")\n",
    "  plt.plot(*zip(*test_scores), label=\"test\")\n",
    "  plt.legend()\n",
    "  plt.grid()\n",
    "  \n",
    "  plt.ioff()\n",
    "  plt.savefig(os.path.join(path, \"plot.png\"))\n",
    "  \n",
    "  if is_ipython:\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "  \"\"\"\n",
    "  Save model, hyperparameters, and training info.\n",
    "  \"\"\"\n",
    "  # save model\n",
    "  torch.save({\n",
    "    \"policy_net\": policy_net.state_dict()\n",
    "  }, os.path.join(path, \"model.pt\"))\n",
    "  \n",
    "  # save hyperparameters\n",
    "  with open(os.path.join(path, \"hparam.json\"), \"w\") as w:\n",
    "    json.dump(hp._asdict(), w, indent=2)\n",
    "  \n",
    "  # save training info\n",
    "  with open(os.path.join(path, \"info.json\"), \"w\") as w:\n",
    "    json.dump(dict([\n",
    "      (\"env\", envname), \n",
    "      (\"action_type\", \"discrete\" if is_discrete_action else \"continuous\"),\n",
    "      (\"test_frames\", test_frames), \n",
    "      (\"test_scores\", test_scores), \n",
    "      (\"steps\", steps), \n",
    "      (\"training_time\", (datetime.now() - start_datetime).seconds)\n",
    "    ]), w, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Testing Function\n",
    "\n",
    "- `test_model`: run policy model in the environment with trained policy network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "  \"\"\"\n",
    "  Test policy model and return the result in training variables. \n",
    "  \"\"\"\n",
    "  # testing variables\n",
    "  frames = []\n",
    "  scores = []\n",
    "  \n",
    "  # repeat for TEST_EPISODES episodes\n",
    "  for _ in range(1, hp.TEST_EPISODES + 1):\n",
    "    # initialize environment and state\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(np.array(state), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "    score = 0\n",
    "    \n",
    "    # start an episode\n",
    "    for frame in count():\n",
    "      # select action\n",
    "      with torch.no_grad():\n",
    "        _, _, action, _ = select_action(state)\n",
    "      \n",
    "      # act to next state\n",
    "      observation, reward, terminated, truncated, _ = env.step(action)\n",
    "      score += reward\n",
    "      done = terminated or truncated\n",
    "      \n",
    "      # update state\n",
    "      state = torch.tensor(np.array(observation), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "      # check end condition\n",
    "      if done:\n",
    "        frames.append(frame)\n",
    "        scores.append(score)\n",
    "        break\n",
    "      \n",
    "  # add to training variables\n",
    "  return np.mean(np.array(frames)), np.mean(np.array(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### REINFORCE\n",
    "\n",
    "In training, simulate agent in the environment to create trajectory, and trains policy network. \n",
    "\n",
    "Object of REINFORCE algorithm is maximizing objective function, which is defined as below. \n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "J_i(\\theta_i) = E_{s0 \\sim p0} \\left[ v_{\\pi_{\\theta_i}}(s_0) \\right] = E_{\\tau \\sim \\pi_\\theta} \\left[ G(\\tau) \\right] \\notag \\\\\n",
    "\\text{where } \\tau = S_0, A_0, R_0, S_1, ..., S_{T-1}, A_{T-1}, R_{T-1}, S_T \\notag \\\\\n",
    "\\text{where } G(\\tau) = R_0 + \\gamma R_1 + ... + \\gamma^{T-1} R_{T-1} \\notag\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "We can get objective function using some mathematical tricks. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\theta J(\\theta) &= \\nabla_\\theta E_{\\tau \\sim \\pi_\\theta} \\left[ G(\\tau) \\right] \\\\ \n",
    "&= E_{\\tau \\sim \\pi_\\theta} \\left[ \\nabla_\\theta \\ln p(\\tau|\\pi_\\theta) G(\\tau) \\right] \\\\ \n",
    "&= E_{\\tau \\sim \\pi_\\theta} \\left[ G(\\tau) \\sum_{t=0}^T \\nabla_\\theta \\ln \\pi(A_t|S_t;\\pi_\\theta) \\right] \\\\ \n",
    "&= E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\gamma^t R_t \\sum_{t=0}^T \\nabla_\\theta \\ln \\pi(A_t|S_t;\\pi_\\theta) \\right] \\\\\n",
    "&= E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\left( \\left( \\sum_{k=t}^{T-1} \\gamma^k R_k \\right) \\nabla_\\theta \\ln \\pi(A_t|S_t;\\pi_\\theta) \\right) \\right] \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The last equation makes sense because the following can be shown: $ E_{\\tau \\sim \\pi_\\theta} \\left[\\gamma^k R_k \\nabla_\\theta \\ln \\pi(A_t | S_t ; \\pi_\\theta) \\right] = 0 \\text{ for } k < t $\n",
    "\n",
    "In this point, we approximate $ \\gamma $-included term to prevent reward vanishing. \n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\sum_{k=t}^{T-1} \\gamma^k R_k \n",
    "= \\gamma^t \\sum_{k=t}^{T-1}  \\gamma^{k-t} R_k \n",
    "= \\gamma^t G_t\n",
    "\\cong G_t \\notag \\\\\n",
    "\\nabla_\\theta J(\\theta) \n",
    "\\cong E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T G_t \\nabla_\\theta \\ln \\pi(A_t|S_t;\\pi_\\theta) \\right] \\notag\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "So, we update parameters as below. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta &\\leftarrow \\theta + \\alpha \\sum_{t=0}^T G_t \\nabla_\\theta \\ln \\pi(A_t|S_t;\\pi_\\theta)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Actor-Critic\n",
    "\n",
    "In REINFORCE algorithm, $ G_t $ is required for training. \n",
    "Thus, training can be done only after the end of an episode, which causes high variance. \n",
    "\n",
    "So, Actor-Critic tries to estimate $ G_t $ using neural network $ Q_w $, or $ V_w $. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\theta J(\\theta) \n",
    "&\\cong E_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T G_t \\nabla_\\theta \\ln \\pi(A_t|S_t; \\pi_\\theta) \\right] \\\\\n",
    "&= \\sum_{t=0}^T E_{S_0,...,A_t} \\left[ \\nabla_\\theta \\ln \\pi(A_t|S_t;\\pi_\\theta) E_{S_{t+1},...,S_T} \\left[ G_t | S_0,..., A_t \\right] \\right] \\\\\n",
    "&=\\sum_{t=0}^T E_{S_0,...,A_t} \\left[ \\nabla_\\theta \\ln \\pi(A_t|S_t;\\pi_\\theta) Q(S_t, A_t) \\right] \\\\\n",
    "&=\\sum_{t=0}^T E_{S_t,A_t} \\left[ \\nabla_\\theta \\ln \\pi(A_t|S_t;\\pi_\\theta) Q(S_t, A_t) \\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "First, we can change average of sums to sum of averages. \n",
    "Then, average for all trajectory can be splitted before $ t $ and after $ t $. \n",
    "Finally, we can define value function $ Q(S_t, A_t) = E_{S_{t+1},...,S_T} \\left[ G_t | S_0,..., A_t \\right] $\n",
    "\n",
    "Update formula for actor $ \\theta $ and critic $ w $ are shown below. \n",
    "Objective of the actor is increasing objective function $ J(\\theta) $. \n",
    "Objective of the critic is decreasing TD error $ \\delta = R_t + \\gamma V_w(S_{t+1}) - V_w(S_t) $\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta &\\leftarrow \\theta + \\alpha  \\nabla_\\theta \\ln \\pi(A_t|S_t;\\pi_\\theta) ( R_t + \\gamma \\max_A Q_w(S_{t+1}, A) - Q_w(S_t, A_t) ) \\\\\n",
    "w &\\leftarrow w - \\beta \\nabla_w (R_t + \\gamma \\max_A Q_w(S_{t+1}, A) - Q_w(S_t, A_t))^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In this code, I used Actor-Critic using $ Q(S_t, A_t) $, so it can't deal with continuous action space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\NamSaeng\\Documents\\Machine Learning\\projects\\RL-Tutorial\\reinforce.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NamSaeng/Documents/Machine%20Learning/projects/RL-Tutorial/reinforce.ipynb#X22sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m observation, reward, terminated, truncated, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NamSaeng/Documents/Machine%20Learning/projects/RL-Tutorial/reinforce.ipynb#X22sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(observation[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/NamSaeng/Documents/Machine%20Learning/projects/RL-Tutorial/reinforce.ipynb#X22sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mmean(observation[:, :, \u001b[39m1\u001b[39;49m]) \u001b[39m>\u001b[39m \u001b[39m185.0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NamSaeng/Documents/Machine%20Learning/projects/RL-Tutorial/reinforce.ipynb#X22sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m   \u001b[39m# penalize green\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NamSaeng/Documents/Machine%20Learning/projects/RL-Tutorial/reinforce.ipynb#X22sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m   reward \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.05\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/NamSaeng/Documents/Machine%20Learning/projects/RL-Tutorial/reinforce.ipynb#X22sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "File \u001b[1;32mc:\\Users\\NamSaeng\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gymnasium\\wrappers\\frame_stack.py:83\u001b[0m, in \u001b[0;36mLazyFrames.__getitem__\u001b[1;34m(self, int_or_slice)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(int_or_slice, \u001b[39mint\u001b[39m):\n\u001b[0;32m     81\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_decompress(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_frames[int_or_slice])  \u001b[39m# single frame\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mstack(\n\u001b[1;32m---> 83\u001b[0m     [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_decompress(f) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_frames[int_or_slice]], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m     84\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# create training directory\n",
    "os.makedirs(path)\n",
    "\n",
    "for episode in tqdm(range(1, hp.NUM_EPISODES + 1)):\n",
    "  # initialize environment and state\n",
    "  state, _ = env.reset()\n",
    "  state = torch.tensor(np.array(state), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "  score = 0\n",
    "  if hp.ALGORITHM == \"REINFORCE\":\n",
    "    reward_history = [] # list of scalars\n",
    "    log_prob_history = [] # list of (1,) tensors\n",
    "  \n",
    "  # start an episode\n",
    "  for frame in count():\n",
    "    # select action\n",
    "    state_value, action_value, action, log_prob = select_action(state)\n",
    "    steps += 1\n",
    "    \n",
    "    # act to next state\n",
    "    observation, reward, terminated, truncated, _ = env.step(action)\n",
    "    score += reward\n",
    "    done = terminated or truncated\n",
    "    if hp.ALGORITHM == \"REINFORCE\":\n",
    "      reward_history.append(reward)\n",
    "      log_prob_history.append(log_prob)\n",
    "    \n",
    "    # update state to next state\n",
    "    state = torch.tensor(np.array(observation), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "    \n",
    "    # Actor-Critic training\n",
    "    if hp.ALGORITHM == \"AC\":\n",
    "      # TD error\n",
    "      current_action_value = action_value.gather(1, torch.tensor([[action]], device=device, dtype=torch.int64))\n",
    "      _, next_action_value, _, _ = select_action(state)\n",
    "      next_action_value = next_action_value.max(1, keepdim=True).values\n",
    "      expected_action_value = reward + hp.GAMMA * (1 - done) * next_action_value\n",
    "      TD = expected_action_value - current_action_value\n",
    "      \n",
    "      # actor loss\n",
    "      actor_loss = -torch.sum(torch.mul(TD, log_prob.unsqueeze(1)))\n",
    "      train_actor_losses.append((steps, actor_loss.item()))\n",
    "      \n",
    "      # critic loss\n",
    "      critic_loss = HuberLoss(reduction=\"none\")(current_action_value, expected_action_value)\n",
    "      train_critic_losses.append((steps, critic_loss.item()))\n",
    "      \n",
    "      # gradient ascent/descent\n",
    "      loss = actor_loss + critic_loss\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    \n",
    "    # check end condition\n",
    "    if done:\n",
    "      train_scores.append((episode, score))\n",
    "      train_frames.append((episode, frame))\n",
    "      break\n",
    "  \n",
    "  # REINFORCE training\n",
    "  if hp.ALGORITHM == \"REINFORCE\":\n",
    "    # get list of G_t\n",
    "    g_history = [0 for _ in range(len(reward_history))]\n",
    "    g_history[-1] = reward_history[-1]\n",
    "    for i in range(len(reward_history) - 2, -1, -1):\n",
    "      g_history[i] = reward_history[i] + g_history[i + 1] * hp.GAMMA\n",
    "    g_history = torch.tensor(g_history, device=device)\n",
    "    \n",
    "    # normalize g_history\n",
    "    train_objectives.append((episode, g_history[0].item()))\n",
    "    g_history = (g_history - torch.mean(g_history)) / torch.std(g_history)\n",
    "    \n",
    "    # get list of log probs of actions\n",
    "    log_prob_history = torch.cat(log_prob_history).to(device=device)\n",
    "    \n",
    "    # gradient ascent\n",
    "    loss = -torch.sum(torch.mul(g_history, log_prob_history))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  \n",
    "  if episode % hp.TEST_FREQ == 0:\n",
    "    mean_frame, mean_score = test_model()\n",
    "    test_frames.append((episode, mean_frame))\n",
    "    test_scores.append((episode, mean_score))\n",
    "    save_plot()\n",
    "    save_model()\n",
    "\n",
    "save_plot()\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "In this block, trained agent plays in the environment. We can see rendered environment played by the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(envname, render_mode=\"human\", continuous=continuous)\n",
    "env = preprocess_env(env)\n",
    "\n",
    "scores = []\n",
    "\n",
    "# if you want to load from trained model, edit this (editable)\n",
    "load_dirname = None\n",
    "\n",
    "if load_dirname is not None:\n",
    "  # load models\n",
    "  path = os.path.join(os.getcwd(), \"reinforce_based\", load_dirname)\n",
    "  checkpoint = torch.load(os.path.join(path, \"model.pt\"), map_location=device)\n",
    "  \n",
    "  policy_net.load_state_dict(checkpoint[\"policy_net\"])\n",
    "\n",
    "# repeat for TEST_EPISODES episodes\n",
    "for episode in range(1, hp.TEST_EPISODES + 1):\n",
    "  # initialize environment and state\n",
    "  state, _ = env.reset()\n",
    "  state = torch.tensor(np.array(state), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "  score = 0\n",
    "  \n",
    "  # start an episode\n",
    "  for _ in count():\n",
    "    # select greedy action\n",
    "    with torch.no_grad():\n",
    "      _, _, action, _ = select_action(state)\n",
    "    \n",
    "    # act to next state\n",
    "    observation, reward, terminated, truncated, _ = env.step(action)\n",
    "    score += reward\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    # update state\n",
    "    state = torch.tensor(np.array(observation), device=device, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    # check end condition\n",
    "    if done:\n",
    "      print(f\"Episode {episode}: {score}\")\n",
    "      scores.append(score)\n",
    "      break\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(f\"Average: {sum(scores) / hp.TEST_EPISODES}\")\n",
    "print(f\"Max: {max(scores)}\")\n",
    "print(f\"Min: {min(scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
